{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Language Model Algebra: Projections + Constraints\n",
    "\n",
    "This notebook explores how n-gram projections and schema-based constraints represent two sides of the same algebraic framework for composing and controlling language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Unified View\n",
    "\n",
    "Both projects implement different forms of **language model algebra**:\n",
    "\n",
    "1. **N-gram Projections**: Algebraic operations on model *inputs* and *combinations*\n",
    "   - Projections transform context before feeding to models\n",
    "   - Models combine through mixture operations (+, *, >>)\n",
    "   - Focus: How models see and process input\n",
    "\n",
    "2. **Schema Constraints (guidedgen)**: Algebraic operations on model *outputs*\n",
    "   - Schemas constrain token generation through logit masking\n",
    "   - Schemas combine through set operations (union, intersection)\n",
    "   - Focus: What models are allowed to generate\n",
    "\n",
    "Together, they form a complete **input → model → output** algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from typing import Dict, List, Set, Optional, Callable\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "# Import our n-gram projection system\n",
    "from ngram_projections.models.base import LanguageModel\n",
    "from ngram_projections.models.ngram import NGramModel\n",
    "from ngram_projections.projections.recency import RecencyProjection\n",
    "from ngram_projections.projections.semantic import SemanticProjection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual Bridge: Schemas as Output Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual implementation showing how schemas work like output projections\n",
    "\n",
    "class OutputProjection:\n",
    "    \"\"\"Base class for output projections (constraints).\"\"\"\n",
    "    \n",
    "    def project_logits(self, logits: Dict[str, float], state: dict) -> Dict[str, float]:\n",
    "        \"\"\"Project/constrain the output distribution.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SchemaProjection(OutputProjection):\n",
    "    \"\"\"Project outputs to match a schema (like guidedgen).\"\"\"\n",
    "    \n",
    "    def __init__(self, valid_tokens_fn: Callable):\n",
    "        self.valid_tokens_fn = valid_tokens_fn\n",
    "    \n",
    "    def project_logits(self, logits: Dict[str, float], state: dict) -> Dict[str, float]:\n",
    "        # Get valid tokens based on current state\n",
    "        valid_tokens = self.valid_tokens_fn(state)\n",
    "        \n",
    "        # Mask invalid tokens (set to -inf)\n",
    "        return {\n",
    "            token: (prob if token in valid_tokens else 0.0)\n",
    "            for token, prob in logits.items()\n",
    "        }\n",
    "\n",
    "class BiasProjection(OutputProjection):\n",
    "    \"\"\"Bias certain tokens in the output.\"\"\"\n",
    "    \n",
    "    def __init__(self, bias_weights: Dict[str, float]):\n",
    "        self.bias_weights = bias_weights\n",
    "    \n",
    "    def project_logits(self, logits: Dict[str, float], state: dict) -> Dict[str, float]:\n",
    "        # Apply bias weights\n",
    "        biased = {}\n",
    "        for token, prob in logits.items():\n",
    "            bias = self.bias_weights.get(token, 1.0)\n",
    "            biased[token] = prob * bias\n",
    "        \n",
    "        # Renormalize\n",
    "        total = sum(biased.values())\n",
    "        return {k: v/total for k, v in biased.items() if total > 0}\n",
    "\n",
    "# Algebraic operations on output projections\n",
    "class UnionProjection(OutputProjection):\n",
    "    \"\"\"Union of valid tokens from multiple projections.\"\"\"\n",
    "    \n",
    "    def __init__(self, projections: List[OutputProjection]):\n",
    "        self.projections = projections\n",
    "    \n",
    "    def project_logits(self, logits: Dict[str, float], state: dict) -> Dict[str, float]:\n",
    "        # Union: token is valid if ANY projection allows it\n",
    "        result = {token: 0.0 for token in logits}\n",
    "        \n",
    "        for proj in self.projections:\n",
    "            projected = proj.project_logits(logits, state)\n",
    "            for token, prob in projected.items():\n",
    "                result[token] = max(result[token], prob)\n",
    "        \n",
    "        return result\n",
    "\n",
    "class IntersectionProjection(OutputProjection):\n",
    "    \"\"\"Intersection of valid tokens from multiple projections.\"\"\"\n",
    "    \n",
    "    def __init__(self, projections: List[OutputProjection]):\n",
    "        self.projections = projections\n",
    "    \n",
    "    def project_logits(self, logits: Dict[str, float], state: dict) -> Dict[str, float]:\n",
    "        # Intersection: token is valid if ALL projections allow it\n",
    "        result = logits.copy()\n",
    "        \n",
    "        for proj in self.projections:\n",
    "            projected = proj.project_logits(logits, state)\n",
    "            result = {\n",
    "                token: min(result.get(token, 0), projected.get(token, 0))\n",
    "                for token in result\n",
    "            }\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Pipeline: Input → Model → Output Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlgebraicLanguageModel:\n",
    "    \"\"\"Complete algebraic model with input and output projections.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model: LanguageModel,\n",
    "                 input_projection=None,\n",
    "                 output_projection=None):\n",
    "        self.model = model\n",
    "        self.input_projection = input_projection\n",
    "        self.output_projection = output_projection\n",
    "    \n",
    "    def predict(self, context: List[str], state: dict = None) -> Dict[str, float]:\n",
    "        # Step 1: Apply input projection\n",
    "        if self.input_projection:\n",
    "            context = self.input_projection.project(context)\n",
    "        \n",
    "        # Step 2: Get model predictions\n",
    "        logits = self.model.predict(context)\n",
    "        \n",
    "        # Step 3: Apply output projection\n",
    "        if self.output_projection:\n",
    "            logits = self.output_projection.project_logits(logits, state or {})\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        \"\"\"Mixture of models.\"\"\"\n",
    "        if isinstance(other, AlgebraicLanguageModel):\n",
    "            return MixtureAlgebraicModel([self, other], [0.5, 0.5])\n",
    "        return NotImplemented\n",
    "    \n",
    "    def __mul__(self, weight: float):\n",
    "        \"\"\"Weighted model for mixtures.\"\"\"\n",
    "        return WeightedAlgebraicModel(self, weight)\n",
    "    \n",
    "    def __matmul__(self, projection):\n",
    "        \"\"\"Apply projection with @ operator.\"\"\"\n",
    "        if hasattr(projection, 'project'):  # Input projection\n",
    "            return AlgebraicLanguageModel(\n",
    "                self.model, \n",
    "                projection,\n",
    "                self.output_projection\n",
    "            )\n",
    "        elif hasattr(projection, 'project_logits'):  # Output projection\n",
    "            return AlgebraicLanguageModel(\n",
    "                self.model,\n",
    "                self.input_projection,\n",
    "                projection\n",
    "            )\n",
    "        return NotImplemented\n",
    "\n",
    "class MixtureAlgebraicModel(AlgebraicLanguageModel):\n",
    "    \"\"\"Mixture of multiple algebraic models.\"\"\"\n",
    "    \n",
    "    def __init__(self, models: List[AlgebraicLanguageModel], weights: List[float]):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "        assert abs(sum(weights) - 1.0) < 1e-6, \"Weights must sum to 1\"\n",
    "    \n",
    "    def predict(self, context: List[str], state: dict = None) -> Dict[str, float]:\n",
    "        result = {}\n",
    "        \n",
    "        for model, weight in zip(self.models, self.weights):\n",
    "            pred = model.predict(context, state)\n",
    "            for token, prob in pred.items():\n",
    "                result[token] = result.get(token, 0) + weight * prob\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: JSON Generation with N-gram Grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a JSON schema constraint\n",
    "def json_object_validator(state: dict) -> Set[str]:\n",
    "    \"\"\"Returns valid tokens for JSON object generation.\"\"\"\n",
    "    depth = state.get('depth', 0)\n",
    "    in_string = state.get('in_string', False)\n",
    "    \n",
    "    if in_string:\n",
    "        # Any token except quotes (simplified)\n",
    "        return {t for t in \"abcdefghijklmnopqrstuvwxyz0123456789 \"}\n",
    "    \n",
    "    if depth == 0:\n",
    "        return {'{'}  # Must start with opening brace\n",
    "    \n",
    "    # Simplified: allow keys, values, and closing brace\n",
    "    return {'\"', ':', ',', '}'}\n",
    "\n",
    "# Create models\n",
    "ngram = NGramModel(n=3)\n",
    "ngram.train([\"{\", '\"name\"', \":\", '\"John\"', \",\", '\"age\"', \":\", \"30\", \"}\"])\n",
    "ngram.train([\"{\", '\"id\"', \":\", '\"123\"', \",\", '\"type\"', \":\", '\"user\"', \"}\"])\n",
    "\n",
    "# Create a mock LLM that tends to generate valid JSON\n",
    "class MockJSONLLM(LanguageModel):\n",
    "    def predict(self, context: List[str]) -> Dict[str, float]:\n",
    "        # Simplified: prefer JSON-like tokens\n",
    "        if context and context[-1] == '{':\n",
    "            return {'\"': 0.9, 'null': 0.1}\n",
    "        elif context and context[-1] == '\"':\n",
    "            return {'name': 0.3, 'id': 0.3, 'type': 0.2, 'age': 0.2}\n",
    "        elif context and context[-1] == ':':\n",
    "            return {'\"': 0.5, '123': 0.25, '30': 0.25}\n",
    "        else:\n",
    "            return {',': 0.4, '}': 0.3, ':': 0.3}\n",
    "\n",
    "llm = MockJSONLLM()\n",
    "\n",
    "# Create projections\n",
    "recency = RecencyProjection(max_suffix_len=3)\n",
    "json_schema = SchemaProjection(json_object_validator)\n",
    "\n",
    "# Compose: N-gram with recency + LLM, both constrained by JSON schema\n",
    "model = ((0.3 * AlgebraicLanguageModel(ngram) @ recency) + \n",
    "         (0.7 * AlgebraicLanguageModel(llm))) @ json_schema\n",
    "\n",
    "# Generate\n",
    "context = ['{']\n",
    "state = {'depth': 1, 'in_string': False}\n",
    "\n",
    "print(\"Generating JSON with n-gram grounding and schema constraints:\")\n",
    "print(\"Context:\", context)\n",
    "print(\"Predictions:\", model.predict(context, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Unification: Categories and Functors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algebraic structure can be formalized using category theory\n",
    "\n",
    "@dataclass\n",
    "class LanguageModelCategory:\n",
    "    \"\"\"\n",
    "    Category where:\n",
    "    - Objects: Language models\n",
    "    - Morphisms: Projections/transformations\n",
    "    - Composition: Function composition\n",
    "    - Identity: Identity projection\n",
    "    \"\"\"\n",
    "    \n",
    "    class Object:\n",
    "        \"\"\"A language model as an object in the category.\"\"\"\n",
    "        def __init__(self, model: LanguageModel):\n",
    "            self.model = model\n",
    "    \n",
    "    class Morphism:\n",
    "        \"\"\"A projection/transformation as a morphism.\"\"\"\n",
    "        def __init__(self, transform: Callable):\n",
    "            self.transform = transform\n",
    "        \n",
    "        def __rshift__(self, other):\n",
    "            \"\"\"Compose morphisms.\"\"\"\n",
    "            return LanguageModelCategory.Morphism(\n",
    "                lambda x: other.transform(self.transform(x))\n",
    "            )\n",
    "    \n",
    "    @staticmethod\n",
    "    def identity():\n",
    "        \"\"\"Identity morphism.\"\"\"\n",
    "        return LanguageModelCategory.Morphism(lambda x: x)\n",
    "\n",
    "# Functors between categories\n",
    "class ProjectionFunctor:\n",
    "    \"\"\"Maps from token sequences to projected sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, projection):\n",
    "        self.projection = projection\n",
    "    \n",
    "    def map_object(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"Apply projection to tokens.\"\"\"\n",
    "        return self.projection.project(tokens)\n",
    "    \n",
    "    def map_morphism(self, f: Callable) -> Callable:\n",
    "        \"\"\"Lift a function on tokens to work on projected tokens.\"\"\"\n",
    "        return lambda x: f(self.map_object(x))\n",
    "\n",
    "class SchemaFunctor:\n",
    "    \"\"\"Maps from distributions to constrained distributions.\"\"\"\n",
    "    \n",
    "    def __init__(self, schema):\n",
    "        self.schema = schema\n",
    "    \n",
    "    def map_object(self, logits: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"Apply schema constraints to logits.\"\"\"\n",
    "        return self.schema.project_logits(logits, {})\n",
    "    \n",
    "    def map_morphism(self, f: Callable) -> Callable:\n",
    "        \"\"\"Lift a function on logits to work on constrained logits.\"\"\"\n",
    "        return lambda x: self.map_object(f(x))\n",
    "\n",
    "print(\"Category Theory Formalization:\")\n",
    "print(\"- Objects: Language Models\")\n",
    "print(\"- Morphisms: Projections (input) and Constraints (output)\")\n",
    "print(\"- Composition: Function composition (>> operator)\")\n",
    "print(\"- Identity: No projection/constraint\")\n",
    "print(\"\\nThis gives us a principled algebraic framework!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Integration: Reliable Wikipedia-Grounded Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaGroundedModel:\n",
    "    \"\"\"\n",
    "    Combines:\n",
    "    1. N-gram model trained on Wikipedia for factual grounding\n",
    "    2. LLM for fluent generation\n",
    "    3. Schema constraints for structured output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, wikipedia_ngram, llm, fact_weight=0.4):\n",
    "        self.wiki_ngram = wikipedia_ngram\n",
    "        self.llm = llm\n",
    "        self.fact_weight = fact_weight\n",
    "        \n",
    "        # Define schema for factual statements\n",
    "        self.fact_schema = self._create_fact_schema()\n",
    "    \n",
    "    def _create_fact_schema(self):\n",
    "        \"\"\"Schema for generating factual statements.\"\"\"\n",
    "        def fact_validator(state: dict) -> Set[str]:\n",
    "            # Ensure generation follows fact pattern\n",
    "            if state.get('generating_fact'):\n",
    "                # Prefer tokens from Wikipedia n-gram\n",
    "                return state.get('wiki_tokens', set())\n",
    "            return set()  # All tokens valid otherwise\n",
    "        \n",
    "        return SchemaProjection(fact_validator)\n",
    "    \n",
    "    def generate_fact(self, topic: str, context: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Generate a fact about the topic, grounded in Wikipedia.\n",
    "        \"\"\"\n",
    "        # Step 1: Find relevant Wikipedia context\n",
    "        wiki_context = self._find_wiki_context(topic, context)\n",
    "        \n",
    "        # Step 2: Get Wikipedia n-gram predictions\n",
    "        wiki_pred = self.wiki_ngram.predict(wiki_context)\n",
    "        \n",
    "        # Step 3: Get LLM predictions\n",
    "        llm_pred = self.llm.predict(context)\n",
    "        \n",
    "        # Step 4: Mix with Wikipedia bias\n",
    "        mixed = {}\n",
    "        all_tokens = set(wiki_pred.keys()) | set(llm_pred.keys())\n",
    "        \n",
    "        for token in all_tokens:\n",
    "            wiki_p = wiki_pred.get(token, 0.0)\n",
    "            llm_p = llm_pred.get(token, 0.0)\n",
    "            \n",
    "            # Boost Wikipedia tokens for factual grounding\n",
    "            if wiki_p > 0.01:  # Token appears in Wikipedia\n",
    "                mixed[token] = self.fact_weight * wiki_p + (1 - self.fact_weight) * llm_p * 1.5\n",
    "            else:\n",
    "                mixed[token] = (1 - self.fact_weight) * llm_p\n",
    "        \n",
    "        # Step 5: Apply schema constraints\n",
    "        state = {'generating_fact': True, 'wiki_tokens': set(wiki_pred.keys())}\n",
    "        constrained = self.fact_schema.project_logits(mixed, state)\n",
    "        \n",
    "        # Normalize\n",
    "        total = sum(constrained.values())\n",
    "        if total > 0:\n",
    "            constrained = {k: v/total for k, v in constrained.items()}\n",
    "        \n",
    "        return constrained\n",
    "    \n",
    "    def _find_wiki_context(self, topic: str, context: List[str]) -> List[str]:\n",
    "        \"\"\"Find relevant Wikipedia context for the topic.\"\"\"\n",
    "        # Simplified: use topic words as context\n",
    "        return topic.lower().split() + context[-3:]\n",
    "\n",
    "# Example usage\n",
    "wiki_ngram = NGramModel(n=3)\n",
    "# Train on \"Wikipedia\" data\n",
    "wiki_ngram.train(\"Albert Einstein was a theoretical physicist\".split())\n",
    "wiki_ngram.train(\"Einstein developed the theory of relativity\".split())\n",
    "wiki_ngram.train(\"The theory of relativity revolutionized physics\".split())\n",
    "\n",
    "class SimpleLLM(LanguageModel):\n",
    "    def predict(self, context: List[str]) -> Dict[str, float]:\n",
    "        # Simple mock LLM\n",
    "        return {\n",
    "            \"was\": 0.2, \"is\": 0.15, \"developed\": 0.15,\n",
    "            \"the\": 0.1, \"theory\": 0.1, \"of\": 0.1,\n",
    "            \"relativity\": 0.05, \"physics\": 0.05, \"scientist\": 0.1\n",
    "        }\n",
    "\n",
    "llm = SimpleLLM()\n",
    "grounded_model = WikipediaGroundedModel(wiki_ngram, llm, fact_weight=0.6)\n",
    "\n",
    "# Generate a fact\n",
    "result = grounded_model.generate_fact(\n",
    "    topic=\"Einstein\",\n",
    "    context=[\"Albert\", \"Einstein\"]\n",
    ")\n",
    "\n",
    "print(\"Wikipedia-Grounded Generation:\")\n",
    "print(\"Topic: Einstein\")\n",
    "print(\"Context: ['Albert', 'Einstein']\")\n",
    "print(\"\\nTop predictions (Wikipedia-grounded):\")\n",
    "for token, prob in sorted(result.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"  {token}: {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complete Algebra: Composition Laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the algebraic laws that our system satisfies\n",
    "\n",
    "class AlgebraicLaws:\n",
    "    \"\"\"\n",
    "    Laws that hold in our language model algebra.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def commutativity_of_mixture():\n",
    "        \"\"\"A + B = B + A for model mixtures.\"\"\"\n",
    "        return \"model1 + model2 == model2 + model1\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def associativity_of_mixture():\n",
    "        \"\"\"(A + B) + C = A + (B + C) for model mixtures.\"\"\"\n",
    "        return \"(model1 + model2) + model3 == model1 + (model2 + model3)\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def distributivity_of_projection():\n",
    "        \"\"\"P @ (A + B) = (P @ A) + (P @ B) for input projections.\"\"\"\n",
    "        return \"projection @ (model1 + model2) == (projection @ model1) + (projection @ model2)\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def composition_of_projections():\n",
    "        \"\"\"(P1 >> P2) @ M = P1 @ (P2 @ M) for projection composition.\"\"\"\n",
    "        return \"(proj1 >> proj2) @ model == proj1 @ (proj2 @ model)\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def identity_projection():\n",
    "        \"\"\"I @ M = M for identity projection.\"\"\"\n",
    "        return \"identity @ model == model\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def schema_intersection_associativity():\n",
    "        \"\"\"(S1 ∩ S2) ∩ S3 = S1 ∩ (S2 ∩ S3) for schema intersection.\"\"\"\n",
    "        return \"(schema1 & schema2) & schema3 == schema1 & (schema2 & schema3)\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def schema_union_associativity():\n",
    "        \"\"\"(S1 ∪ S2) ∪ S3 = S1 ∪ (S2 ∪ S3) for schema union.\"\"\"\n",
    "        return \"(schema1 | schema2) | schema3 == schema1 | (schema2 | schema3)\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def de_morgans_law():\n",
    "        \"\"\"¬(S1 ∪ S2) = ¬S1 ∩ ¬S2 for schema complement.\"\"\"\n",
    "        return \"not (schema1 | schema2) == (not schema1) & (not schema2)\"\n",
    "\n",
    "print(\"Algebraic Laws of Language Model Composition:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "laws = AlgebraicLaws()\n",
    "for law_name in dir(laws):\n",
    "    if not law_name.startswith('_'):\n",
    "        law_fn = getattr(laws, law_name)\n",
    "        if callable(law_fn):\n",
    "            print(f\"\\n{law_name.replace('_', ' ').title()}:\")\n",
    "            print(f\"  {law_fn()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"These laws enable reasoning about complex model compositions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified Framework Summary\n",
    "\n",
    "The combination of **n-gram projections** and **schema constraints** gives us a complete algebraic framework:\n",
    "\n",
    "### Input Algebra (N-gram Projections)\n",
    "- **Recency**: Project to longest matching suffix\n",
    "- **Semantic**: Project using embeddings\n",
    "- **Edit Distance**: Find similar contexts\n",
    "- **Composition**: Chain projections with `>>`\n",
    "\n",
    "### Model Algebra (Mixtures)\n",
    "- **Addition**: Equal-weight mixture `model1 + model2`\n",
    "- **Scaling**: Weighted contribution `0.3 * model`\n",
    "- **Union**: Fallback/ensemble `model1 | model2`\n",
    "\n",
    "### Output Algebra (Schema Constraints)\n",
    "- **Intersection**: All constraints must hold `schema1 & schema2`\n",
    "- **Union**: Any constraint can hold `schema1 | schema2`\n",
    "- **Switch**: Discriminated unions based on fields\n",
    "- **Sequence**: Different schemas per position\n",
    "\n",
    "### The Complete Pipeline\n",
    "```python\n",
    "model = (\n",
    "    (0.3 * ngram @ recency_projection) +     # Grounded n-gram\n",
    "    (0.7 * llm @ semantic_projection)        # Semantic LLM\n",
    ") @ json_schema                              # Output constraints\n",
    "```\n",
    "\n",
    "This unified algebra enables:\n",
    "1. **Reliable generation** through constraints\n",
    "2. **Factual grounding** through n-gram biasing\n",
    "3. **Continuous learning** via n-gram updates\n",
    "4. **Interpretable control** over model behavior\n",
    "5. **Composable building blocks** for complex systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}