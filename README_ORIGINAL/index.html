<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A Calculus for Language Models - Mathematical framework for compositional language modeling"><meta name=author content="LangCalc Team"><link href=https://github.com/queelius/langcalc/README_ORIGINAL/ rel=canonical><link rel=icon href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.21"><title>Autoregressive Models: Inductive Biases and Projections - LangCalc Documentation</title><link rel=stylesheet href=../assets/stylesheets/main.2a3383ac.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M1%207.775V2.75C1%201.784%201.784%201%202.75%201h5.025c.464%200%20.91.184%201.238.513l6.25%206.25a1.75%201.75%200%200%201%200%202.474l-5.026%205.026a1.75%201.75%200%200%201-2.474%200l-6.25-6.25A1.75%201.75%200%200%201%201%207.775m1.5%200c0%20.066.026.13.073.177l6.25%206.25a.25.25%200%200%200%20.354%200l5.025-5.025a.25.25%200%200%200%200-.354l-6.25-6.25a.25.25%200%200%200-.177-.073H2.75a.25.25%200%200%200-.25.25ZM6%205a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E');--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2.5%201.75v11.5c0%20.138.112.25.25.25h3.17a.75.75%200%200%201%200%201.5H2.75A1.75%201.75%200%200%201%201%2013.25V1.75C1%20.784%201.784%200%202.75%200h8.5C12.216%200%2013%20.784%2013%201.75v7.736a.75.75%200%200%201-1.5%200V1.75a.25.25%200%200%200-.25-.25h-8.5a.25.25%200%200%200-.25.25m13.274%209.537zl-4.557%204.45a.75.75%200%200%201-1.055-.008l-1.943-1.95a.75.75%200%200%201%201.062-1.058l1.419%201.425%204.026-3.932a.75.75%200%201%201%201.048%201.074M4.75%204h4.5a.75.75%200%200%201%200%201.5h-4.5a.75.75%200%200%201%200-1.5M4%207.75A.75.75%200%200%201%204.75%207h2a.75.75%200%200%201%200%201.5h-2A.75.75%200%200%201%204%207.75%22/%3E%3C/svg%3E');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M0%208a8%208%200%201%201%2016%200A8%208%200%200%201%200%208m8-6.5a6.5%206.5%200%201%200%200%2013%206.5%206.5%200%200%200%200-13M6.5%207.75A.75.75%200%200%201%207.25%207h1a.75.75%200%200%201%20.75.75v2.75h.25a.75.75%200%200%201%200%201.5h-2a.75.75%200%200%201%200-1.5h.25v-2h-.25a.75.75%200%200%201-.75-.75M8%206a1%201%200%201%201%200-2%201%201%200%200%201%200%202%22/%3E%3C/svg%3E');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M3.499.75a.75.75%200%200%201%201.5%200v.996C5.9%202.903%206.793%203.65%207.662%204.376l.24.202c-.036-.694.055-1.422.426-2.163C9.1.873%2010.794-.045%2012.622.26%2014.408.558%2016%201.94%2016%204.25c0%201.278-.954%202.575-2.44%202.734l.146.508.065.22c.203.701.412%201.455.476%202.226.142%201.707-.4%203.03-1.487%203.898C11.714%2014.671%2010.27%2015%208.75%2015h-6a.75.75%200%200%201%200-1.5h1.376a4.5%204.5%200%200%201-.563-1.191%203.84%203.84%200%200%201-.05-2.063%204.65%204.65%200%200%201-2.025-.293.75.75%200%200%201%20.525-1.406c1.357.507%202.376-.006%202.698-.318l.009-.01a.747.747%200%200%201%201.06%200%20.75.75%200%200%201-.012%201.074c-.912.92-.992%201.835-.768%202.586.221.74.745%201.337%201.196%201.621H8.75c1.343%200%202.398-.296%203.074-.836.635-.507%201.036-1.31.928-2.602-.05-.603-.216-1.224-.422-1.93l-.064-.221c-.12-.407-.246-.84-.353-1.29a2.4%202.4%200%200%201-.507-.441%203.1%203.1%200%200%201-.633-1.248.75.75%200%200%201%201.455-.364c.046.185.144.436.31.627.146.168.353.305.712.305.738%200%201.25-.615%201.25-1.25%200-1.47-.95-2.315-2.123-2.51-1.172-.196-2.227.387-2.706%201.345-.46.92-.27%201.774.019%203.062l.042.19.01.05c.348.443.666.949.94%201.553a.75.75%200%201%201-1.365.62c-.553-1.217-1.32-1.94-2.3-2.768L6.7%205.527c-.814-.68-1.75-1.462-2.692-2.619a3.7%203.7%200%200%200-1.023.88c-.406.495-.663%201.036-.722%201.508.116.122.306.21.591.239.388.038.797-.06%201.032-.19a.75.75%200%200%201%20.728%201.31c-.515.287-1.23.439-1.906.373-.682-.067-1.473-.38-1.879-1.193L.75%205.677V5.5c0-.984.48-1.94%201.077-2.664.46-.559%201.05-1.055%201.673-1.353z%22/%3E%3C/svg%3E');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M13.78%204.22a.75.75%200%200%201%200%201.06l-7.25%207.25a.75.75%200%200%201-1.06%200L2.22%209.28a.75.75%200%200%201%20.018-1.042.75.75%200%200%201%201.042-.018L6%2010.94l6.72-6.72a.75.75%200%200%201%201.06%200%22/%3E%3C/svg%3E');--md-admonition-icon--question:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M0%208a8%208%200%201%201%2016%200A8%208%200%200%201%200%208m8-6.5a6.5%206.5%200%201%200%200%2013%206.5%206.5%200%200%200%200-13M6.92%206.085h.001a.749.749%200%201%201-1.342-.67c.169-.339.436-.701.849-.977C6.845%204.16%207.369%204%208%204a2.76%202.76%200%200%201%201.637.525c.503.377.863.965.863%201.725%200%20.448-.115.83-.329%201.15-.205.307-.47.513-.692.662-.109.072-.22.138-.313.195l-.006.004a6%206%200%200%200-.26.16%201%201%200%200%200-.276.245.75.75%200%200%201-1.248-.832c.184-.264.42-.489.692-.661q.154-.1.313-.195l.007-.004c.1-.061.182-.11.258-.161a1%201%200%200%200%20.277-.245C8.96%206.514%209%206.427%209%206.25a.61.61%200%200%200-.262-.525A1.27%201.27%200%200%200%208%205.5c-.369%200-.595.09-.74.187a1%201%200%200%200-.34.398M9%2011a1%201%200%201%201-2%200%201%201%200%200%201%202%200%22/%3E%3C/svg%3E');--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M6.457%201.047c.659-1.234%202.427-1.234%203.086%200l6.082%2011.378A1.75%201.75%200%200%201%2014.082%2015H1.918a1.75%201.75%200%200%201-1.543-2.575Zm1.763.707a.25.25%200%200%200-.44%200L1.698%2013.132a.25.25%200%200%200%20.22.368h12.164a.25.25%200%200%200%20.22-.368Zm.53%203.996v2.5a.75.75%200%200%201-1.5%200v-2.5a.75.75%200%200%201%201.5%200M9%2011a1%201%200%201%201-2%200%201%201%200%200%201%202%200%22/%3E%3C/svg%3E');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2.344%202.343za8%208%200%200%201%2011.314%2011.314A8.002%208.002%200%200%201%20.234%2010.089a8%208%200%200%201%202.11-7.746m1.06%2010.253a6.5%206.5%200%201%200%209.108-9.275%206.5%206.5%200%200%200-9.108%209.275M6.03%204.97%208%206.94l1.97-1.97a.749.749%200%200%201%201.275.326.75.75%200%200%201-.215.734L9.06%208l1.97%201.97a.749.749%200%200%201-.326%201.275.75.75%200%200%201-.734-.215L8%209.06l-1.97%201.97a.749.749%200%200%201-1.275-.326.75.75%200%200%201%20.215-.734L6.94%208%204.97%206.03a.75.75%200%200%201%20.018-1.042.75.75%200%200%201%201.042-.018%22/%3E%3C/svg%3E');--md-admonition-icon--danger:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M9.504.43a1.516%201.516%200%200%201%202.437%201.713L10.415%205.5h2.123c1.57%200%202.346%201.909%201.22%203.004l-7.34%207.142a1.25%201.25%200%200%201-.871.354h-.302a1.25%201.25%200%200%201-1.157-1.723L5.633%2010.5H3.462c-1.57%200-2.346-1.909-1.22-3.004zm1.047%201.074L3.286%208.571A.25.25%200%200%200%203.462%209H6.75a.75.75%200%200%201%20.694%201.034l-1.713%204.188%206.982-6.793A.25.25%200%200%200%2012.538%207H9.25a.75.75%200%200%201-.683-1.06l2.008-4.418.003-.006-.004-.009-.006-.006-.008-.001q-.005%200-.009.004%22/%3E%3C/svg%3E');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M4.72.22a.75.75%200%200%201%201.06%200l1%20.999a3.5%203.5%200%200%201%202.441%200l.999-1a.748.748%200%200%201%201.265.332.75.75%200%200%201-.205.729l-.775.776c.616.63.995%201.493.995%202.444v.327q0%20.15-.025.292c.408.14.764.392%201.029.722l1.968-.787a.75.75%200%200%201%20.556%201.392L13%207.258V9h2.25a.75.75%200%200%201%200%201.5H13v.5q-.002.615-.141%201.186l2.17.868a.75.75%200%200%201-.557%201.392l-2.184-.873A5%205%200%200%201%208%2016a5%205%200%200%201-4.288-2.427l-2.183.873a.75.75%200%200%201-.558-1.392l2.17-.868A5%205%200%200%201%203%2011v-.5H.75a.75.75%200%200%201%200-1.5H3V7.258L.971%206.446a.75.75%200%200%201%20.558-1.392l1.967.787c.265-.33.62-.583%201.03-.722a1.7%201.7%200%200%201-.026-.292V4.5c0-.951.38-1.814.995-2.444L4.72%201.28a.75.75%200%200%201%200-1.06m.53%206.28a.75.75%200%200%200-.75.75V11a3.5%203.5%200%201%200%207%200V7.25a.75.75%200%200%200-.75-.75ZM6.173%205h3.654A.17.17%200%200%200%2010%204.827V4.5a2%202%200%201%200-4%200v.327c0%20.096.077.173.173.173%22/%3E%3C/svg%3E');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5%205.782V2.5h-.25a.75.75%200%200%201%200-1.5h6.5a.75.75%200%200%201%200%201.5H11v3.282l3.666%205.76C15.619%2013.04%2014.543%2015%2012.767%2015H3.233c-1.776%200-2.852-1.96-1.899-3.458Zm-2.4%206.565a.75.75%200%200%200%20.633%201.153h9.534a.75.75%200%200%200%20.633-1.153L12.225%2010.5h-8.45ZM9.5%202.5h-3V6c0%20.143-.04.283-.117.403L4.73%209h6.54L9.617%206.403A.75.75%200%200%201%209.5%206Z%22/%3E%3C/svg%3E');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M1.75%202.5h10.5a.75.75%200%200%201%200%201.5H1.75a.75.75%200%200%201%200-1.5m4%205h8.5a.75.75%200%200%201%200%201.5h-8.5a.75.75%200%200%201%200-1.5m0%205h8.5a.75.75%200%200%201%200%201.5h-8.5a.75.75%200%200%201%200-1.5M2.5%207.75v6a.75.75%200%200%201-1.5%200v-6a.75.75%200%200%201%201.5%200%22/%3E%3C/svg%3E');}</style><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config",""),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#autoregressive-models-inductive-biases-and-projections class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title="LangCalc Documentation" class="md-header__button md-logo" aria-label="LangCalc Documentation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> LangCalc Documentation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Autoregressive Models: Inductive Biases and Projections </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/queelius/langcalc title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> queelius/langcalc </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../getting-started/ class=md-tabs__link> Getting Started </a> </li> <li class=md-tabs__item> <a href=../projection-system/ class=md-tabs__link> Projection System </a> </li> <li class=md-tabs__item> <a href=../user-guide/ class=md-tabs__link> User Guide </a> </li> <li class=md-tabs__item> <a href=../api/ class=md-tabs__link> API Reference </a> </li> <li class=md-tabs__item> <a href=../advanced/ class=md-tabs__link> Advanced Topics </a> </li> <li class=md-tabs__item> <a href=../development/ class=md-tabs__link> Development </a> </li> <li class=md-tabs__item> <a href=../about/ class=md-tabs__link> About </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title="LangCalc Documentation" class="md-nav__button md-logo" aria-label="LangCalc Documentation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> LangCalc Documentation </label> <div class=md-nav__source> <a href=https://github.com/queelius/langcalc title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> queelius/langcalc </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=../getting-started/ class="md-nav__link "> <span class=md-ellipsis> Getting Started </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Getting Started </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../getting-started/installation/ class=md-nav__link> <span class=md-ellipsis> Installation </span> </a> </li> <li class=md-nav__item> <a href=../getting-started/quickstart/ class=md-nav__link> <span class=md-ellipsis> Quick Start </span> </a> </li> <li class=md-nav__item> <a href=../getting-started/concepts/ class=md-nav__link> <span class=md-ellipsis> Core Concepts </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../projection-system/ class="md-nav__link "> <span class=md-ellipsis> Projection System </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Projection System </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../projection-system/overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../projection-system/formalism/ class=md-nav__link> <span class=md-ellipsis> Mathematical Formalism </span> </a> </li> <li class=md-nav__item> <a href=../projection-system/augmentations/ class=md-nav__link> <span class=md-ellipsis> Canonical Augmentations </span> </a> </li> <li class=md-nav__item> <a href=../projection-system/ordering/ class=md-nav__link> <span class=md-ellipsis> Ordering Principles </span> </a> </li> <li class=md-nav__item> <a href=../projection-system/implementation/ class=md-nav__link> <span class=md-ellipsis> Reference Implementation </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../user-guide/ class="md-nav__link "> <span class=md-ellipsis> User Guide </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> User Guide </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../user-guide/models/ class=md-nav__link> <span class=md-ellipsis> Language Models </span> </a> </li> <li class=md-nav__item> <a href=../user-guide/algebra/ class=md-nav__link> <span class=md-ellipsis> Algebraic Operations </span> </a> </li> <li class=md-nav__item> <a href=../user-guide/transformations/ class=md-nav__link> <span class=md-ellipsis> Context Transformations </span> </a> </li> <li class=md-nav__item> <a href=../user-guide/examples/ class=md-nav__link> <span class=md-ellipsis> Examples & Patterns </span> </a> </li> <li class=md-nav__item> <a href=../user-guide/best-practices/ class=md-nav__link> <span class=md-ellipsis> Best Practices </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../api/ class="md-nav__link "> <span class=md-ellipsis> API Reference </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> API Reference </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../api/core/ class=md-nav__link> <span class=md-ellipsis> Core Module </span> </a> </li> <li class=md-nav__item> <a href=../api/models/ class=md-nav__link> <span class=md-ellipsis> Models </span> </a> </li> <li class=md-nav__item> <a href=../api/projections/ class=md-nav__link> <span class=md-ellipsis> Projections </span> </a> </li> <li class=md-nav__item> <a href=../api/augmentations/ class=md-nav__link> <span class=md-ellipsis> Augmentations </span> </a> </li> <li class=md-nav__item> <a href=../api/algebra/ class=md-nav__link> <span class=md-ellipsis> Algebra </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6> <div class="md-nav__link md-nav__container"> <a href=../advanced/ class="md-nav__link "> <span class=md-ellipsis> Advanced Topics </span> </a> <label class="md-nav__link " for=__nav_6 id=__nav_6_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Advanced Topics </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../advanced/suffix-arrays/ class=md-nav__link> <span class=md-ellipsis> Suffix Arrays </span> </a> </li> <li class=md-nav__item> <a href=../advanced/grounding/ class=md-nav__link> <span class=md-ellipsis> Lightweight Grounding </span> </a> </li> <li class=md-nav__item> <a href=../advanced/performance/ class=md-nav__link> <span class=md-ellipsis> Performance Optimization </span> </a> </li> <li class=md-nav__item> <a href=../advanced/extending/ class=md-nav__link> <span class=md-ellipsis> Extending LangCalc </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_7> <div class="md-nav__link md-nav__container"> <a href=../development/ class="md-nav__link "> <span class=md-ellipsis> Development </span> </a> <label class="md-nav__link " for=__nav_7 id=__nav_7_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Development </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../development/contributing/ class=md-nav__link> <span class=md-ellipsis> Contributing </span> </a> </li> <li class=md-nav__item> <a href=../development/testing/ class=md-nav__link> <span class=md-ellipsis> Testing </span> </a> </li> <li class=md-nav__item> <a href=../development/style/ class=md-nav__link> <span class=md-ellipsis> Code Style </span> </a> </li> <li class=md-nav__item> <a href=../development/releases/ class=md-nav__link> <span class=md-ellipsis> Release Process </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_8> <div class="md-nav__link md-nav__container"> <a href=../about/ class="md-nav__link "> <span class=md-ellipsis> About </span> </a> <label class="md-nav__link " for=__nav_8 id=__nav_8_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_8_label aria-expanded=false> <label class=md-nav__title for=__nav_8> <span class="md-nav__icon md-icon"></span> About </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../about/license/ class=md-nav__link> <span class=md-ellipsis> License </span> </a> </li> <li class=md-nav__item> <a href=../about/changelog/ class=md-nav__link> <span class=md-ellipsis> Changelog </span> </a> </li> <li class=md-nav__item> <a href=../about/paper/ class=md-nav__link> <span class=md-ellipsis> Academic Paper </span> </a> </li> <li class=md-nav__item> <a href=../about/citation/ class=md-nav__link> <span class=md-ellipsis> Citation </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=autoregressive-models-inductive-biases-and-projections>Autoregressive Models: Inductive Biases and Projections<a class=headerlink href=#autoregressive-models-inductive-biases-and-projections title="Permanent link">&para;</a></h1> <h3 id=abstract>Abstract<a class=headerlink href=#abstract title="Permanent link">&para;</a></h3> <p>This paper explores the use of inductive biases and projection functions in autoregressive (AR) models to enhance out-of-distribution (OOD) generalization. We revisit the concept of infini-grams, which leverage suffix arrays to manage arbitrary input (context) lengths efficiently. This approach is compared to traditional <span class=arithmatex>\(n\)</span>-gram models, highlighting its advantages in sample efficiency and computational scalability. We delve into various inductive biases, such as the recency bias, shortest edit distance, and semantic similarity, illustrating their impact on AR model performance. By framing OOD generalization as a projection problem, we propose strategies to optimize these projections through meta-learning and nested optimization. Furthermore, we discuss the integration of classical information retrieval techniques and pre-trained language model embeddings to enhance the semantic relevance of projections. Our findings suggest that combining symbolic AI methods with deep learning representations can yield more interpretable and sample-efficient AR models, with broad applications in natural language processing, code generation, and scientific discovery.</p> <h2 id=introduction>Introduction<a class=headerlink href=#introduction title="Permanent link">&para;</a></h2> <p>The infini-gram model is an autoregressive (AR) model that predicts the next token based on the longest suffix in the training data that matches the input. Essentially, they are finding some <em>projection</em> of the input to the training data to allow the AR model to generate coherent text continuations from inputs it has never seen before. This is known as out-of-distribution (OOD) generalization, where we are trying to generalize to tasks (like predict continuations of an input never seen before) that is not in the training data.</p> <p>Since the model converges in distribution to the data generating process (DGP) as the sample size goes to infinity, the key challenge is to find sample-efficient inductive biases that provide the model with more information about the task or the DGP, allowing it to generalize to OOD data more effectively and with fewer samples.</p> <p>In this paper, we seek to formalize a class of inductive biases as <em>projections</em> of the input onto the training data.</p> <h2 id=ar-models>AR Models<a class=headerlink href=#ar-models title="Permanent link">&para;</a></h2> <p>AR models form a cornerstone in natural language processing, predicting the probability of a word <span class=arithmatex>\(w_t\)</span> given all preceding words <span class=arithmatex>\(w_{&lt;t}\)</span> and the training data <span class=arithmatex>\(D\)</span>:</p> <div class=arithmatex>\[ \Pr\!{}_D\{w_t \mid w_{&lt;t}\}. \]</div> <p>Historically, the prefix <span class=arithmatex>\(w_{&lt;t}\)</span> is limited to a fixed length <span class=arithmatex>\(n\)</span>,</p> <div class=arithmatex>\[ \Pr\!{}_D\{w_t \mid w_{t-n:t}\}, \]</div> <p>where <span class=arithmatex>\(a:b\)</span> denotes the range <span class=arithmatex>\(a, a+1, \ldots, b-1\)</span>.</p> <p>Infini-gram models dynamically adjust the order of the <span class=arithmatex>\(n\)</span>-gram based on the longest suffix in the training data that matches the input:</p> <div class=arithmatex>\[ \Pr\!{}_D\{w_t \mid \operatorname{longest\_suffix}_D(w_{&lt;t})\}, \]</div> <p>where <span class=arithmatex>\(\operatorname{longest\_suffix}_D\)</span> finds the longest suffix of the context <span class=arithmatex>\(w_{&lt;t}\)</span> in the training data <span class=arithmatex>\(D\)</span>.</p> <p>For AR models to generate continuations of the input, <span class=arithmatex>\(\operatorname{longest\_suffix}\)</span> makes a lot of sense. It allows the model to find training data that is both similiar to the input and relevant to the task of predicting the next token from previous tokens.</p> <p>Let's be a bit formal about what <span class=arithmatex>\(\operatorname{longest\_suffix}_D\)</span> represents: it is a kind of <em>projection</em> of the input onto the training data <span class=arithmatex>\(D\)</span>, which is an i.i.d. sample from some (unknown) data generating process (DGP). Let us denote the probability distribution of the DGP as <span class=arithmatex>\(\Pr{}_{\!\theta}\)</span>, where <span class=arithmatex>\(\theta\)</span> are unknown parameters, and the probability distribution of the AR model as <span class=arithmatex>\(\Pr{}_{\!\hat\theta}\)</span>, where <span class=arithmatex>\(\hat\theta\)</span> are the estimated parameters of the AR model based on the training data <span class=arithmatex>\(D\)</span>.</p> <p>The goal of the AR model is to estimate <span class=arithmatex>\(\theta\)</span> from the training data, which will allow it to generalize to new data that the DGP would plausibly produce. A paricularly useful task is to predict what the DGP would plausibly produce <em>given</em> some input <span class=arithmatex>\(w_{&lt;t}\)</span>, where <span class=arithmatex>\(w_{&lt;t}\)</span> is a sequence of tokens that the DGP has produced so far and may represent some task of interest, like "What is the solution to \&lt;math problem&gt;?"</p> <p>The distribution of <span class=arithmatex>\(w_{t:t+k}\)</span> conditioned on <span class=arithmatex>\(w_{&lt;t}\)</span> is given by</p> <div class=arithmatex>\[ \Pr{}_{\!\theta}\{w_{t:t+k} \mid w_{&lt;t}\} = \frac{\Pr{}_{\!\theta}\{w_{1:(t+k)}\}}{\Pr{}_{\!\theta}\{w_{1:t}\}}, \]</div> <p>where <span class=arithmatex>\(w_{a:b}\)</span> is a sub-sequence of tokens produced by the DGP from time <span class=arithmatex>\(a\)</span> to time <span class=arithmatex>\(b\)</span> (time is a <em>logical time</em> that just implies some ordering). The primary task is often to <em>generate</em> plausible continuations of the input, for which there are many possible <em>sampling</em> strategies to do this, like beam search, top-<span class=arithmatex>\(k\)</span> sampling, and nucleus sampling, all of which use the conditional probability distribution to generate continuations one token at a time. This approach is justfied by the chain rule of probability:</p> <div class=arithmatex>\[ \Pr{}_{\!\theta}\{w_t \mid w_{&lt;t}\} = \prod_{i=1}^t \Pr{}_\theta\{w_i \mid w_{&lt;i}\}. \]</div> <p>Notice that when we generate continuations of the input, we are not trying to find a sequence that <em>maximizes</em> the conditional probability:</p> <div class=arithmatex>\[ w_{t:(t+k)}^* = \arg\max_{w_{t:(t+k)}} \Pr{}_{\!\theta}\{w_{t:(t+k)} \mid w_{&lt;t}\}, \]</div> <p>but rather we are <em>sampling</em> from the distribution. We identify a few justifications for doing this:</p> <ol> <li> <p>The DGP <span class=arithmatex>\(\Pr{}_\theta\)</span> is often stochastic and we capture this stochasticity in our predictions or continuations. However, even if the DGP is not stochastic, we only have an uncertain estimate <span class=arithmatex>\(\Pr{}_{\!\hat\theta}\)</span> conditioned on data <span class=arithmatex>\(D\)</span> randomly sampled from data by the DGP. So, sampling from it is a way of generating continuations that reflect the uncertainty. See Appendix F: Bootstrapping the Sampling Distribution for a more rigorous way to estimate uncertainty in the model as opposed to the DGP.</p> </li> <li> <p>There is a trade-off between exploration and exploitation, where the model needs to balance between generating plausible continuations and exploring new possibilities.</p> </li> <li> <p>Finding the most likely sequence of tokens is NP-hard, so we often resort to approximate methods like greedily sampling from the conditional distribution one token at at time, or using more accurate but computationally expensive methods like beam search to find more likely sequences of tokens.</p> </li> </ol> <p>Since we do know know the DGP <span class=arithmatex>\(\Pr_{\!\theta}\)</span>, we replace it with our AR model based on a training data <span class=arithmatex>\(D\)</span>, <span class=arithmatex>\(\Pr{}_{\!\hat\theta}\)</span>, and use the AR model to approximate the DGP. As the sample size goes to infinity, by the law of large numbers, the empirical distribution of the training data will converge to the true distribution of the DGP:</p> <div class=arithmatex>\[ \Pr{}_{\!\hat\theta}\{w_t \mid w_{&lt;t}\} \rightarrow_d \Pr{}_{\!\theta}\{w_t \mid w_{&lt;t}\}. \]</div> <p>The Infini-gram model converges in distribution to the DGP, but we do not have <em>infinite</em> data. Thus, since virtually all inputs have never been senn before, we are interested in finding ways to allow the model to generalize <em>out-of-distribution</em> (OOD). On the task of next-token prediction, this means generating continuations of the input that the DGP would plausibly produce but are not in the training data.</p> <p>This is a key challenge in machine learning. Ideally, we want the AR model to generate plausible continuations of any input from very small amounts of training data <span class=arithmatex>\(D\)</span>. A primary way to do this is to <em>constrain</em> or <em>bias</em>, which we call an <em>inductive bias</em>.</p> <p>The projection function <span class=arithmatex>\(\operatorname{longest\_suffix}_D\)</span> is an example of an inductive bias. It is a way for the model to find the most relevant part of the training data to the input to give it some ability to generalize OOD on the task of generating plausible continuations of the input.</p> <p>We formalize this idea of projection as an inductive bias and discuss how it can be used to improve the sample efficiency of both <span class=arithmatex>\(n\)</span>-gram models and AR models, like transformers, LSTMs, and RNNs.</p> <h2 id=inductive-biases>Inductive Biases<a class=headerlink href=#inductive-biases title="Permanent link">&para;</a></h2> <p>Given two learning algorithms, <span class=arithmatex>\(A\)</span> and <span class=arithmatex>\(B\)</span>, if <span class=arithmatex>\(A\)</span> requires fewer samples to do well on a task than <span class=arithmatex>\(B\)</span>, then <span class=arithmatex>\(A\)</span> is more sample-efficient than <span class=arithmatex>\(B\)</span> on that task. In the context of <span class=arithmatex>\(n\)</span>-gram models, the task is to predict the next token given a sequence of previous tokens. One way to improve sample efficiency is to choose an inductive bias that provides the model with more information about the task or the DGP, allowing it to generalize to OOD data more effectively and with fewer samples.</p> <p>The <span class=arithmatex>\(\operatorname{longest\_suffix}_D\)</span> projection is an inductive bias that we might label the <em>recency bias</em>. The recency bias has some advantages:</p> <ol> <li> <p>It is computationally efficient, as shown by the suffix array data structure used in the infini-gram model. It only requires a linear scan of the training data to find the longest suffix. This scalability is crucial for training on large datasets, as the time complexity of the recency bias is <span class=arithmatex>\(O(n)\)</span>, where <span class=arithmatex>\(n\)</span> is the length of the context.</p> </li> <li> <p>It corresponds to a simple inductive bias that is easy to understand, implement, and justify. If the future is like the past, then the most recent past is often the most relevant data point. This is particularly relevant for tasks like language modeling, where the context is often a sequence of words that are related to each other in a temporal order and in which the most recent words are often the most relevant for predicting the next word.</p> </li> </ol> <p>The recency bias may not always help to find the most relevant context in the training data, e.g., the most relevant context may be at the start of a document. However, even when the most relevnat context is the most recent, the <span class=arithmatex>\(\operatorname{longest\_suffix}\)</span> may fail to properly use it. For example, if the context is <code>the dog ran after the</code> and we ask it to predict the next word, but the training data only contains <code>the dog chased the cat</code>, the longest suffix is the highly uninformative word <code>the</code>. We see that the naive longest suffix match fails to take into account slight variations, even if those slight variations have essentially identical meanings.</p> <p>These challenges suggest some possible inductive biases that can be used to improve the OOD generalization of <span class=arithmatex>\(\Pr{}_{\hat\theta}\)</span>. We consider the set of inductive biases that can be formulated as <em>projections</em> of the input (context) onto the training data. Let us formally write down the problem of OOD generalization in the context of AR models as a projection problem:</p> <p>$$ \Pr!{}<em _t="<t">D{w_t \mid \operatorname{proj}_D(w</em>)}, $$ where <span class=arithmatex>\(\operatorname{proj}_D\)</span> is a function that maps the input <span class=arithmatex>\(w_{&lt;t}\)</span> to a subset of the training data <span class=arithmatex>\(D\)</span> that is most relevant for producing continuations of the <span class=arithmatex>\(w_{&lt;t}\)</span> that the DGP would <em>likely</em> produce.</p> <h2 id=learing-the-projection-function>Learing the Projection Function<a class=headerlink href=#learing-the-projection-function title="Permanent link">&para;</a></h2> <p>Let us parameterize the projection function as</p> <div class=arithmatex>\[ \mathcal{F} = \{ \operatorname{proj}_D(x; \beta) \mid \beta \in \mathcal{B} \}, \]</div> <p>where <span class=arithmatex>\(\beta\)</span> is an index or label that specifies the projection. For example, <span class=arithmatex>\(\beta = 1\)</span> could be a label for <span class=arithmatex>\(\operatorname{longest\_suffix}_D\)</span>, or it could be something more complicated based on the space of possible projections <span class=arithmatex>\(\mathcal{F}\)</span>.</p> <p>We can choose a projection function from <span class=arithmatex>\(\mathcal{F}\)</span> by choosing a <span class=arithmatex>\(\beta\)</span> in <span class=arithmatex>\(\mathcal{B}\)</span>, which is frequently a discrete set of possible projections.</p> <p>We choose the projection function in one or two ways:</p> <ul> <li> <p>Utilize domain-knowledge expertise (hand-crafted feature engineering). By lessons of the bitter kind, we observe that this appraoch often does not scale with increasing compute and data, as it requires human expertise that is often scarce and limited.</p> </li> <li> <p>Treat it as an optimization (search or learning) problem, where <span class=arithmatex>\(\beta\)</span> is a tunable parameter of the model.</p> </li> </ul> <p>The second approach is more general and can be used to optimize the projection function based on the data <span class=arithmatex>\(D\)</span> and the task we are measuring performance on. Note that because the projection function <span class=arithmatex>\(\operatorname{proj}_D(\cdot;\beta)\)</span> is intended to improve OOD generalization performance, we do not optimize it on the training data <span class=arithmatex>\(D\)</span> but on a held-out test data <span class=arithmatex>\(D'\)</span>.</p> <p>The optimization problem is then conceptualized as an iterated two-stage process.</p> <ol> <li> <p><strong>Initialize:</strong> Set <span class=arithmatex>\(i\)</span> to <span class=arithmatex>\(1\)</span> and choose a <span class=arithmatex>\(\beta_0\)</span> based on prior knowledge.</p> </li> <li> <p><strong>Stage 1:</strong> Optimize the parameters of the AR model <span class=arithmatex>\(\theta\)</span> on the training data <span class=arithmatex>\(D\)</span> using <span class=arithmatex>\(\operatorname{proj}_D(\cdot;\beta_{i-1})\)</span>:</p> <div class=arithmatex>\[ \hat\theta_i = \arg\max_{\theta} \prod_{t=1}^T \Pr{}_{\!\theta}(w_t \mid \operatorname{proj}_{D}(w_{&lt;t}; \beta_{i-1})). \]</div> </li> <li> <p><strong>Stage 2:</strong> Optimize the parameters of the projection function indexed by <span class=arithmatex>\(\beta_i\)</span> on the test data <span class=arithmatex>\(D'\)</span> using the AR model indexed by <span class=arithmatex>\(\hat\theta_i\)</span>: $$ \hat\beta_i = \arg\max_{\beta} \prod_{t=1}^T \Pr{}<em d_="D'">{!\hat\theta_i}(w_t \mid \operatorname{proj}</em>; \hat\beta_i)), $$ where }(w_{&lt;t<span class=arithmatex>\(D'\)</span> is test data <span class=arithmatex>\(D'\)</span> (e.g., held-out test data) used to estimate the quality of the projection function.</p> </li> <li> <p><strong>Convergence Test:</strong> If the parameters <span class=arithmatex>\(\hat\theta_i\)</span> and <span class=arithmatex>\(\hat\beta_i\)</span> have converged, stop. Otherwise, set <span class=arithmatex>\(i = i + 1\)</span> and go to step 2 (Stage 1).</p> </li> </ol> <p>To mitigate overfitting on the test data, we can use strategies like early stopping, where we stop the optimization process before convergence, or choose different test data at each iteration.</p> <p>If the set of projection functions do not affect the performance of the AR model on the training data <span class=arithmatex>\(D\)</span>, then convergence is obtained after one iteration. Since the parameters <span class=arithmatex>\(\beta\)</span> and <span class=arithmatex>\(\theta\)</span> are usually disjoint (they do no share parameters), the primary way in which a projection function can affect the performance of the AR model on the training data is by changing the distribution of the training data that the AR model sees. For instance, <span class=arithmatex>\(\beta\)</span> may include a parameter that limits the maximum length of the context, which can change the parameters of the AR model that are estimated from the data.</p> <p>Next, we consider the space of possible projection functions <span class=arithmatex>\(\mathcal{F}\)</span>.</p> <h2 id=hypothesis-space-of-projection-functions-inductive-biases>Hypothesis Space of Projection Functions (Inductive Biases)<a class=headerlink href=#hypothesis-space-of-projection-functions-inductive-biases title="Permanent link">&para;</a></h2> <p>To formalize notation, we denote the space of projection functions <span class=arithmatex>\(\mathcal{F}\)</span> with the type</p> <div class=arithmatex>\[ \mathcal{T}^* \mapsto \mathcal{T}^*, \]</div> <p>where <span class=arithmatex>\(\mathcal{T}\)</span> are the set of <em>tokens</em> (words, characters, etc.) and <span class=arithmatex>\(\mathcal{T}^*\)</span> is the set of sequences of tokens. The projection function <span class=arithmatex>\(\operatorname{proj}_D\)</span> maps a sequence of tokens to another sequence of tokens, which is (ideally) a subset of the training data <span class=arithmatex>\(D\)</span> such that the Infini-gram model can generate plausible continuations of the input based on suffix matches in the training data.</p> <p>This space is of course too large to search over, so we need to make some assumptions about the structure of the space of projection functions. We can consider a few simple projection functions that can be used to improve the sample efficiency of AR models:</p> <ol> <li> <p><strong>Recency Bias:</strong> The recency bias is a simple projection function that finds the longest suffix of the input in the training data. It is a kind of <em>greedy</em> projection that assumes the most recent tokens are the most relevant for predicting the next token. The recency bias is a simple and computationally efficient inductive bias that can be used to improve the sample efficiency of AR models. This is the <em>default</em> behavior of the Infini-gram model, and by construction all other projection functions incorporate the recency bias.</p> </li> <li> <p><strong>Similarity Bias:</strong> The similarity bias is a more complex projection function that finds the most similar sequence in the training data to the input. Because this could distort the input too much, we constrain the similarity bias to only apply so-called suffix extensions to the left.</p> </li> </ol> <p>We can draw inspiration from techniques developed in information retrieval (IR), natural language processing (NLP), and classical AI informed search strategies to design projections (inductive biases) that yield more sample efficient algorithms that improve OOD generalization. It is worth pointing out that in high-dimensional spaces, essentially every input is OOD, so designing effective inductive biases (projections) is crucial for generalization.</p> <p>Since the longest suffix projection function is already given, in the next section we consider ways to extend approximations of the input suffix to find longer and potentially more relevant context in the training data.</p> <h2 id=extending-the-suffix>Extending The Suffix<a class=headerlink href=#extending-the-suffix title="Permanent link">&para;</a></h2> <p>When we project the input onto the training data and obtain the longest matching suffix, we necessarily lose information about the rest of the input.</p> <p>We have a predictive model, the Infini-gram model itself, that can be used to go extend the suffix in a way that projects onto the training data.</p> <p>Let us formalize this. We have an input <span class=arithmatex>\(w_{&lt;t}\)</span> and a training data <span class=arithmatex>\(D\)</span>. We project the input onto the training data to find the longest matching suffix <span class=arithmatex>\(w_{t':t}\)</span> in the training data, where <span class=arithmatex>\(t' \leq t\)</span>.</p> <p>We know that <span class=arithmatex>\(w_{t'-1:t}\)</span> does not match the training data <span class=arithmatex>\(D\)</span> but the suffix <span class=arithmatex>\(w_{t':t}\)</span> does. Thus, <span class=arithmatex>\(w_{t'-1}\)</span> needs to be substituted for a different token for the suffix to have a chance at finding a match in the training data. </p> <p>Let us denote this <span class=arithmatex>\(t'-1\)</span>-th token as <span class=arithmatex>\(w'_{t'-1}\)</span>. It is a random variable that we can sample from the AR model. That is, we can use the AR model to compute the conditional probability of <span class=arithmatex>\(w'_{t'-1}\)</span> given <span class=arithmatex>\(w_{t':t}\)</span> as a way of sampling extensions of the suffix to the left:</p> <div class=arithmatex>\[ \Pr{}_{\!\hat\theta}\{w_{t'-1} \mid w_{t':t}\} = \frac{\Pr{}_{\!\hat\theta}\{w_{t'-1:t}\}}{\Pr{}_{\!\hat\theta}\{w_{t':t}\}}. \]</div> <p>We can compute joint probabilities using the AR model, and thus we can use the AR model to cosnider realizations of <span class=arithmatex>\(w'_{t'-1}\)</span> given <span class=arithmatex>\(w_{t':t}\)</span> that the model (training data) would likely produce.</p> <p>This is mostly a <em>computational</em> trick, since we do not want to randomly sample tokens that are unlikely to be produced by the DGP (and thus unlikely to project onto the training data).</p> <p>We may rewrite the conditional probability as:</p> <div class=arithmatex>\[ \Pr{}_{\!\hat\theta}\{w'_{t'-1} \mid w_{t':t}\} \propto \Pr{}_{\!\hat\theta}\{w_{t'-1}\} \prod_{i=t'}^t \Pr{}_{\!\hat\theta}\{w_i \mid w_{t'-1:i}\}, \]</div> <p>which is something that the Infini-gram model can compute very efficiently. Thus, we can generate the conditional distribution of <span class=arithmatex>\(w_{t'-1}\)</span> given <span class=arithmatex>\(w_{t':t}\)</span> and sample from this distribution to consider suffix extensions.</p> <p>However, we have to have some similarity measure to determine when to stop extending the suffix, as we may end up with a very long suffix that is not very relevant to the input. We can use the earlier similarity measures discussed.</p> <p>We can sample multiple left-extensions of the suffix, compute the similarity of each extension to the input, and use some strategy to either stop extending the suffix or to accept an extension based on the similarity to the input, such as <span class=arithmatex>\(\arg\max\)</span> or sampling based on the similarity.</p> <h3 id=challenges>Challenges<a class=headerlink href=#challenges title="Permanent link">&para;</a></h3> <p>Learning sample efficient representations of the data is the primary driver of OOD generalization. <em>Deep Learning</em> is about learning these representations from the data. We can use pre-trained models like BERT and GPT to learn representations of the data (sequences of tokens), also known as embeddings, that are a more sample-efficient representation than the token sequences in our Infini-gram model.</p> <p>In particular, these embeddings can be used to compute the similarity between tokens. A canonical example is <code>word2vec</code>, which learns an embedding of words that allows a kind of semantic algebra on words such linear combinations of embeddings often result in meaningful embeddings. The canoncial example is:</p> <div class=arithmatex>\[ \operatorname{embed}(\text{king}) - \operatorname{embed}(\text{man}) + \operatorname{embed}(\text{woman}) \approx \operatorname{embed}(\text{queen}). \]</div> <p>We can use these embeddings to compute the semnatic similarity between tokens, and thus try to find suffix extensions of the input that oth retain the meaning of the input and project onto the training data.</p> <h4 -=- id=sequence-embeddings>Sequence Embeddings<a class=headerlink href=#sequence-embeddings title="Permanent link">&para;</a></h4> <p>Suffix extensions using token embeddings like <code>word2vec</code> may be too simplistic, as they operate at the level of atomic tokens. Most of the <em>meaning</em> of a sequence of tokens is in the relationships and order of the tokens, not just the tokens themselves. This is a well-studied problem in NLP, and there are many models that model the semantics of a language, from classical models </p> <h4 -=- id=computational-complexity>Computational Complexity<a class=headerlink href=#computational-complexity title="Permanent link">&para;</a></h4> <p>If we use LLM embeddings, it may be costly to compute the similarity between the input and all segments in the training data. We could, however, take the training data and compute embeddings for each segment and store them in a vector storage database for fast retrieval:</p> <div class=arithmatex>\[ \operatorname{proj}_D(x;\beta) = \arg\max_{y \in \operatorname{segments}_\beta(D)} \operatorname{similarity}_\beta(\operatorname{embed}(x), \operatorname{embed}(y)), \]</div> <p>where <span class=arithmatex>\(\operatorname{embed}\)</span> is a function that maps tokens or sequences to embeddings. Since we have all of the embeddings in a vector storage database, the above <span class=arithmatex>\(\arg\max\)</span> operation can be computed very efficiently at the cost of precomputing and storing the embeddings.</p> <h2 id=uncertainty-estimation>Uncertainty Estimation<a class=headerlink href=#uncertainty-estimation title="Permanent link">&para;</a></h2> <p>While infini-gram models provide point estimates for token probabilities, understanding the uncertainty in these estimates is crucial for robust decision-making and for gaining insights into model confidence. </p> <p>We can apply bootstrapping to Infini-gram models by repeatedly sampling with replacement from the training data to create multiple bootstrap samples. For each sample, we train an Infini-gram model and use it to produce next-token probabilities for a given input. This process allows us to construct confidence intervals for our probability estimates.</p> <p>More precisely, we estimate the <em>sampling</em> distribution of the model estimate by resampling from the data <span class=arithmatex>\(D\)</span>. The bootstrapped sampling distribution is given by $$ {\hat\theta<sup>j}_{j=1}</sup>R, $$ where <span class=arithmatex>\(R\)</span> is the number of resamples (with replacement) from <span class=arithmatex>\(D\)</span> and $$ \hat\theta_b^j = \arg\max_{\theta} \prod_{t=1}^T \Pr{}<em _t="<t">{!D^j}{w_t \mid w</em>}, $$ is the <span class=arithmatex>\(j\)</span>-th estimate based on the resampled data and <span class=arithmatex>\(D^j\)</span> is the <span class=arithmatex>\(j\)</span>-th resample of the data <span class=arithmatex>\(D\)</span>. Since <span class=arithmatex>\(\hat\theta^j\)</span> is an estimate of the model parameters based on the resampled data <span class=arithmatex>\(D^j\)</span>, by the plug-in principle, we can estimate the sampling distribution of the model as $$ \Bigl { \Pr{}<em j=1>{!\hat\theta^j} \Bigr}</em>^R. $$</p> <h3 id=confidence-intervals>Confidence Intervals<a class=headerlink href=#confidence-intervals title="Permanent link">&para;</a></h3> <p>To generate confidence intervals of, say, the predictive distribution of the model, we can sample a model from the sampling distribution and provide the input to the model to produce the set of next-token probabilities. We can do this <span class=arithmatex>\(B\)</span> times to get a set of <span class=arithmatex>\(B\)</span> next-token probabilities, and thus for each next-token, we can generate a confidence interval for its probability.</p> <p>This is not easy to do with the neural language models because they are computationally expensive to train. For the Infini-gram model, we can just resample the documents in the training data <span class=arithmatex>\(D\)</span>.</p> <h3 id=implications-for-llms>Implications for LLMs<a class=headerlink href=#implications-for-llms title="Permanent link">&para;</a></h3> <p>Interestingly, the confidence intervals derived from bootstrapped infini-gram models may provide valuable insights into the uncertainty of larger language models (LLMs) trained on the same data. While LLMs are more complex and capture higher-order dependencies, the fundamental uncertainties present in the training data should affect both types of models. For instance, if an infini-gram model shows wide confidence intervals for certain contexts or token predictions, it suggests high variability or insufficient data in those areas. An LLM trained on the same data might also struggle with these contexts, even if it doesn't explicitly compute confidence intervals. This connection opens up possibilities for using simpler, more interpretable models like infini-grams as proxies for understanding the uncertainties in more complex models. It could provide a computationally efficient way to estimate when an LLM might be less confident, without needing to compute expensive uncertainty estimates directly on the LLM itself.</p> <p>Consider a scenario where both an infini-gram model and an LLM are trained on a corpus of scientific papers. If the infini-gram model shows wide confidence intervals when predicting terms in a specific scientific domain, it might indicate that the LLM should also be less confident when generating content in that domain, even if the LLM doesn't explicitly calculate confidence intervals.</p> <p>While this approach shows promise, it's important to note that the relationship between infini-gram uncertainty and LLM uncertainty is not guaranteed to be straightforward. Factors such as the LLM's ability to leverage long-range dependencies and its more complex training process may lead to divergences. Future work could involve empirically studying the correlation between infini-gram confidence intervals and LLM performance or uncertainty estimates derived through other means.</p> <h2 id=experimental-results>Experimental Results<a class=headerlink href=#experimental-results title="Permanent link">&para;</a></h2> <p>We can compare the performance of the recency bias, shortest edit distance, and semantic similarity bias on a language modeling task. We can use perplexity as a measure of the model's performance on the task, where lower perplexity indicates better performance.</p> <h3 id=python-code>Python Code<a class=headerlink href=#python-code title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=c1># code here</span>
</span></code></pre></div> <p>For more details, see the <a href=https://github.com/queelius/ngram-projections>GitHub repository</a> for this project.</p> <h2 id=conclusion>Conclusion<a class=headerlink href=#conclusion title="Permanent link">&para;</a></h2> <p>We have reframed of OOD generalization in the context of AR models as a context reduction and matching problem and explored various inductive biases to improve sample efficiency.</p> <p>Even hand-crafted inductive biases like the recency bias and similarity bias can significantly enhance AR models' performance, but utilizing learned embeddings from pre-trained models like BERT and GPT can likely yield more effective results.</p> <p>In either case, we see that the goal is to reduce or rewrite the context to increase the probability of finding a match in the training data. This is a discrete optimization problem that can be solved using techniques from information retrieval and natural language processing, and so we can leverage these techniques to design more sample-efficient learning algorithms that search over the space of possible context reductions and rewrites to find the most relevant training data to facilitate OOD generalization.</p> <p>Even if an exact match is found in the training data, we often still want to explore a larger space of possibilities to make new discoveries and generate novel and creative outputs.</p> <p>Further research into optimizing these techniques and seamlessly integrating them into AR frameworks promises to advance natural language processing, driving innovation in computational linguistics and machine learning, and help facilitate the development of more intelligent and creative AI systems that can be more easily explained and understood than current neural models, which are often seen as black boxes with inscrutable decision-making processes.</p> <p>By leveraging a LLMs embeddings for sample efficient representatations and classical symbolic AI techniques for context reduction and matching, we can build more interpretable and efficient AR models that can be used in a wide range of applications, from chatbots to code generation to scientific discovery and beyond.</p> <h2 id=appendices>Appendices<a class=headerlink href=#appendices title="Permanent link">&para;</a></h2> <h3 -=- id=a-reward-functions>A: Reward Functions<a class=headerlink href=#a-reward-functions title="Permanent link">&para;</a></h3> <p>Previously, we discussed the idea of projecting the input onto the training data to find the most relevant context for predicting the next token that the DGP is likely to produce.</p> <p>However, we are normally not interesed in predicting the next token, but <em>biasing</em> the model to generate outputs that are more likely to score well on some task. For example, if the task is to generate a coherent text continuation, we want to bias the model to generate text continuations that are coherent and correct, even if the the DGP, given the input, is more likely to generate very different continuations (e.g., toxic, incoherent, or incorrect).</p> <p>One way to fine-tune the model to generate more effective outputs is to use a reward function that scores the outputs based on some task-specific criteria. A nearly universal approach is to take your reward function and produce <span class=arithmatex>\(k\)</span> samples from the model, then score each sample using the reward function, and then sample these outputs based on their scores, e.g., <span class=arithmatex>\(\Pr{}_{\hat\theta}\{w_{t:(t+k)} \mid w_{&lt;t}\} \propto \exp\{R(w_{1:(t+k)})\}\)</span>.</p> <p>However, what if we want to go beyond predicting the DGP's next token and instead generate outputs that are more effective at solving a particular task?</p> <h3 -=- id=b-data-transformation>B: Data Transformation<a class=headerlink href=#b-data-transformation title="Permanent link">&para;</a></h3> <p>The training data <span class=arithmatex>\(D\)</span> is the primary source of information we have about the DGP. However, the training data may not be in the optimal form for solving the task we have in mind.</p> <p>So, a final inductive bias we can consider is data transformation. We can transform the training data into a more suitable form for solving the task at hand. One way which can be particularly effective at improving the sample efficiency of the model is to transform the training data into a more abstract representation space.</p> <p>There are a lot of fancy things you can do, but in interest of transparency and interpretability, we will consider a simple transformation: stemming or lemmatization.</p> <p>Both of these are computationally efficient techniques that reduce the vocabulary size and thus increase the probability of finding relevant projections of the input onto the training data. They are also simple to implement and understand, making them a good choice for a first pass at data transformation.</p> <p>Essentially, this transformation allows for "reasoning" over more abstract representations of the DGP, facilitating OOD generalization but at a loss of some information and expressiveness.</p> <blockquote> <p>Predictive modeling now takes place over this more abstract representation space, which can be more sample efficient. However, when we generate sequences, if the end product is, say, high-quality text, we may have to decode the stemmed or lemmatized representations back to unstemmed or unlemmatized forms, which can be a challenge.</p> </blockquote> <h3 -=- id=c-other-kinds-of-inductive-biases>C: Other Kinds of Inductive Biases<a class=headerlink href=#c-other-kinds-of-inductive-biases title="Permanent link">&para;</a></h3> <p>We formalized most of our inductive biases as projections of the input onto the training data. However, we can consider other kinds of inductive biases that can be used to improve the sample efficiency of AR models that directly affect the AR model's probabilty distribution.</p> <p>In particular, we can also consider more complex inductive biases that involve pattern matching and context-free grammars. For example, we can use a context-free grammar to define the space of possible continuations of the input.</p> <p>In theory, we could have a number of production rules on the input that restrict the set of possible continuations to some CFG. This is a more hand-crafted inductive bias that requires more domain knowledge and human expertise to implement, but it may be appropriate in some circumstances, such as when the task requires generating text that follows a specific structure or format, like JSON or Python code.</p> <h3 -=- id=d-similarity-bias-shortest-edit-distance>D: Similarity Bias: Shortest Edit Distance<a class=headerlink href=#d-similarity-bias-shortest-edit-distance title="Permanent link">&para;</a></h3> <p>Shortest edit distance finds the shortest sequence of operations (e.g., swaps, insertions, deletions, and substitutions) transforming the current context <span class=arithmatex>\(w_{&lt;t}\)</span> into a sequence in <span class=arithmatex>\(D\)</span>.</p> <p>The recency bias can be seen as a special case of the similarity bias where we only allow deletions from the end of the context until a match is found.</p> <p>A justification for using shortest edit distance is based on the idea of similarity: if two sequences are similar, they are more likely to have similar continuations. Edit distance is a way to measure the similarity between two sequences based on the minimum number of operations needed to transform one into the other.</p> <h4 -=- id=example>Example<a class=headerlink href=#example title="Permanent link">&para;</a></h4> <p>Let's use the earlier example, where we have as input "the dog ran after the" and the training data <span class=arithmatex>\(D\)</span> contains the following:</p> <ol> <li>"a dog chased the cat in the garden"</li> <li>"the dog chased the cat, but the cat climbed a tree and got away"</li> <li>"the mouse ran from the cheese trap after setting it off"</li> </ol> <p>The longest suffix is "the". What is shortest edit distance to find a match in the training data? We can perform the following two edits: substitute "ran" with "chased" and delete "after", resulting in "the dog chased the" which has a longest suffix match equal to "the dog chased the" (2), and so the next token predicted is "cat". Thus, a completion by the model might be: "the dog ran after the cat, but the cat climbed a tree and got away". The training data does not contain this completion, but the shortest edit distance found a <em>relevant</em> projection to the training data.</p> <p>What else could we have found within two edits? We could have substituted "dog" with "mouse" and "after" with "from", resulting in the input "the mouse ran from the" and the completion "the dog ran after the cheese trap after setting it off". This is a less plausible completion for the DGP, and things could go much worse, but we see that just counting the number of edits can lead to a poor projection onto the training data.</p> <h4 -=- id=challenges_1>Challenges<a class=headerlink href=#challenges_1 title="Permanent link">&para;</a></h4> <p>As the example demonstrated, a primary challenge with shortest edit distance is that it treats all single edits as having a uniform cost. Ideally, when we edit the input, we want to preserve the "meaning" of the context. Thus, some edits should be more costly than others, based on how much they change the meaning of the input.</p> <p>Also, the shortest (least-cost) edit distance, particularly when we combine it with non-uniform costs, is more computationally intensive than the longest prefix projection. However, it is tractable, e.g., graph search in GOFAI. Approximate methods like Monte Carlo Tree Search (MCTS) can also be used to find approximate solutions.</p> <h3 -=- id=e-semantic-similarity-least-cost-edit-distance>E: Semantic Similarity: Least-Cost Edit Distance<a class=headerlink href=#e-semantic-similarity-least-cost-edit-distance title="Permanent link">&para;</a></h3> <p>A significant issue with <em>shortest edit distance</em> is that it treats each edit as having a uniform cost (a kind of uninformed search). A simple extension is to add a cost to each edit based on some measure of semantic similarity between tokens or sequences.</p> <p>Once we have a cost in place, we can use classical search techniques, like A* search, to find relevant sequences in the training data to the current context.</p> <p>Classical IR (Information Retrieval) techniques like BM25, query expansion, and semantic similarity measures can be used to assign costs to edits.</p> <ol> <li> <p>Input expansion (query expansion): Expand the input to multiple possible sequences that are similar to the input.</p> </li> <li> <p>Treat the input like a search query in IR and use BM25 or other similarity measures to find the most similar sequences in the training data. We then define the projection function as:</p> </li> </ol> <div class=arithmatex>\[ \operatorname{proj}_D(x;\beta) = \arg\max_{y \in \operatorname{segments}_\beta(D)} \operatorname{similarity}_\beta(x, y), \]</div> <p>where <span class=arithmatex>\(\operatorname{segments}_\beta(D)\)</span> is a segmentation strategy of the data <span class=arithmatex>\(D\)</span> into segments (e.g., sentences paragraphs) and <span class=arithmatex>\(\operatorname{similarity}_\beta\)</span> is a similarity measure between the input <span class=arithmatex>\(x\)</span> and the segment <span class=arithmatex>\(y\)</span>, e.g., cosine similarity or Euclidean distance between embeddings or some tf-idf measure, like BM25.</p> <p>We can use the inferred <span class=arithmatex>\(\beta\)</span> to find the most relevant segments in the training data to the input, and then use the AR model to generate continuations of the input based on these segments.</p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2025 LangCalc Project </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/queelius/langcalc target=_blank rel=noopener title="GitHub Repository" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.indexes", "navigation.top", "search.suggest", "search.highlight", "search.share", "content.code.copy", "content.code.annotate", "toc.follow", "toc.integrate"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script> <script src=../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=../javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>