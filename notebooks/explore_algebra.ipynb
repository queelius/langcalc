{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algebraic Language Model Exploration\n",
    "\n",
    "This notebook explores the algebraic API for composing language models with n-gram projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from ngram_projections.models.base import LanguageModel\n",
    "from ngram_projections.models.ngram import NGramModel\n",
    "from ngram_projections.models.mixture import MixtureModel\n",
    "from ngram_projections.projections.recency import RecencyProjection\n",
    "from ngram_projections.projections.edit_distance import EditDistanceProjection\n",
    "from ngram_projections.projections.semantic import SemanticProjection\n",
    "from ngram_projections.projections.attention import AttentionProjection\n",
    "from ngram_projections.algebra.combinators import compose, ensemble, cascade, memoize\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "\n",
    "# For pretty printing\n",
    "import json\n",
    "def pprint(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        print(json.dumps(obj, indent=2))\n",
    "    else:\n",
    "        print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Model Creation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample training data\n",
    "training_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"The dog chased the cat through the garden\",\n",
    "    \"Natural language processing with neural networks\",\n",
    "    \"Deep learning models for text generation\",\n",
    "    \"The cat sat on the mat in the sunny garden\",\n",
    "    \"Machine learning algorithms process natural language\",\n",
    "    \"The brown dog ran quickly through the park\",\n",
    "    \"Neural networks learn patterns from data\",\n",
    "]\n",
    "\n",
    "# Create and train n-gram models with different orders\n",
    "ngram_2 = NGramModel(n=2)\n",
    "ngram_3 = NGramModel(n=3)\n",
    "ngram_4 = NGramModel(n=4)\n",
    "\n",
    "for text in training_texts:\n",
    "    tokens = text.lower().split()\n",
    "    ngram_2.train(tokens)\n",
    "    ngram_3.train(tokens)\n",
    "    ngram_4.train(tokens)\n",
    "\n",
    "print(\"Trained n-gram models with orders 2, 3, and 4\")\n",
    "print(f\"Bigram model has {len(ngram_2.counts)} unique n-grams\")\n",
    "print(f\"Trigram model has {len(ngram_3.counts)} unique n-grams\")\n",
    "print(f\"4-gram model has {len(ngram_4.counts)} unique n-grams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Algebraic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic predictions\n",
    "context = [\"the\", \"dog\"]\n",
    "\n",
    "print(\"Individual model predictions:\")\n",
    "print(f\"Bigram: {ngram_2.predict(context)}\")\n",
    "print(f\"Trigram: {ngram_3.predict(context)}\")\n",
    "print(f\"4-gram: {ngram_4.predict(context)}\\n\")\n",
    "\n",
    "# Create mixture models using algebraic operations\n",
    "print(\"Mixture models using algebraic operations:\")\n",
    "\n",
    "# Equal weight mixture\n",
    "mixture_equal = ngram_2 + ngram_3\n",
    "print(f\"ngram_2 + ngram_3: {mixture_equal.predict(context)}\")\n",
    "\n",
    "# Weighted mixture\n",
    "mixture_weighted = 0.3 * ngram_2 + 0.7 * ngram_3\n",
    "print(f\"0.3 * ngram_2 + 0.7 * ngram_3: {mixture_weighted.predict(context)}\")\n",
    "\n",
    "# Three-way mixture\n",
    "mixture_three = 0.2 * ngram_2 + 0.5 * ngram_3 + 0.3 * ngram_4\n",
    "print(f\"0.2 * ngram_2 + 0.5 * ngram_3 + 0.3 * ngram_4: {mixture_three.predict(context)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Projection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create projection functions\n",
    "recency = RecencyProjection(max_suffix_len=3)\n",
    "edit_dist = EditDistanceProjection(max_distance=2)\n",
    "semantic = SemanticProjection(embedding_dim=50)\n",
    "attention = AttentionProjection()\n",
    "\n",
    "# Test projections on a longer context\n",
    "long_context = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\"]\n",
    "\n",
    "print(\"Projection results:\")\n",
    "print(f\"Original context: {long_context}\")\n",
    "print(f\"Recency projection: {recency.project(long_context)}\")\n",
    "print(f\"Edit distance projection: {edit_dist.project(long_context)}\")\n",
    "print(f\"Semantic projection: {semantic.project(long_context)}\")\n",
    "print(f\"Attention projection: {attention.project(long_context)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Models with Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply projections to models using @ operator\n",
    "ngram_recency = ngram_3 @ recency\n",
    "ngram_semantic = ngram_3 @ semantic\n",
    "ngram_attention = ngram_3 @ attention\n",
    "\n",
    "# Compare predictions with different projections\n",
    "test_context = [\"natural\", \"language\", \"processing\", \"with\", \"neural\"]\n",
    "\n",
    "print(\"Predictions with different projections:\")\n",
    "print(f\"Context: {test_context}\\n\")\n",
    "print(f\"No projection: {ngram_3.predict(test_context)}\")\n",
    "print(f\"With recency: {ngram_recency.predict(test_context)}\")\n",
    "print(f\"With semantic: {ngram_semantic.predict(test_context)}\")\n",
    "print(f\"With attention: {ngram_attention.predict(test_context)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Composed Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose projections using >> operator\n",
    "recency_then_semantic = recency >> semantic\n",
    "semantic_then_attention = semantic >> attention\n",
    "\n",
    "# Union of projections using | operator\n",
    "recency_or_semantic = recency | semantic\n",
    "\n",
    "# Apply composed projections\n",
    "print(\"Composed projections:\")\n",
    "print(f\"Original context: {test_context}\\n\")\n",
    "print(f\"Recency >> Semantic: {recency_then_semantic.project(test_context)}\")\n",
    "print(f\"Semantic >> Attention: {semantic_then_attention.project(test_context)}\")\n",
    "print(f\"Recency | Semantic: {recency_or_semantic.project(test_context)}\")\n",
    "\n",
    "# Apply to model\n",
    "ngram_complex = ngram_3 @ (recency >> semantic)\n",
    "print(f\"\\nModel with recency>>semantic: {ngram_complex.predict(test_context)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mock LLM Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock LLMs for demonstration\n",
    "from ngram_projections.models.base import LanguageModel\n",
    "\n",
    "class MockLLM(LanguageModel):\n",
    "    def __init__(self, name, bias_tokens=None):\n",
    "        self.name = name\n",
    "        self.bias_tokens = bias_tokens or []\n",
    "    \n",
    "    def predict(self, context: List[str]) -> Dict[str, float]:\n",
    "        # Simulate LLM predictions with some bias\n",
    "        base_probs = {\n",
    "            \"networks\": 0.15,\n",
    "            \"models\": 0.12,\n",
    "            \"algorithms\": 0.10,\n",
    "            \"data\": 0.08,\n",
    "            \"learning\": 0.07,\n",
    "        }\n",
    "        \n",
    "        # Add bias for specific tokens\n",
    "        for token in self.bias_tokens:\n",
    "            base_probs[token] = base_probs.get(token, 0) + 0.2\n",
    "        \n",
    "        # Normalize\n",
    "        total = sum(base_probs.values())\n",
    "        return {k: v/total for k, v in base_probs.items()}\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"MockLLM({self.name})\"\n",
    "\n",
    "# Create different LLMs\n",
    "llm_general = MockLLM(\"general\")\n",
    "llm_technical = MockLLM(\"technical\", bias_tokens=[\"algorithms\", \"networks\"])\n",
    "llm_ml = MockLLM(\"ml-focused\", bias_tokens=[\"learning\", \"models\", \"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. N-gram + LLM Mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mixtures of n-gram and LLM models\n",
    "context = [\"neural\", \"network\"]\n",
    "\n",
    "# Pure models\n",
    "print(\"Individual predictions:\")\n",
    "print(f\"N-gram: {ngram_3.predict(context)}\")\n",
    "print(f\"General LLM: {llm_general.predict(context)}\")\n",
    "print(f\"Technical LLM: {llm_technical.predict(context)}\")\n",
    "print(f\"ML LLM: {llm_ml.predict(context)}\\n\")\n",
    "\n",
    "# Mixtures\n",
    "print(\"Mixture models:\")\n",
    "\n",
    "# N-gram for grounding + LLM for generalization\n",
    "grounded = 0.4 * ngram_3 + 0.6 * llm_general\n",
    "print(f\"0.4 * ngram + 0.6 * general_llm: {grounded.predict(context)}\\n\")\n",
    "\n",
    "# Multiple LLMs with n-gram\n",
    "ensemble = 0.3 * ngram_3 + 0.3 * llm_technical + 0.4 * llm_ml\n",
    "print(f\"0.3 * ngram + 0.3 * technical + 0.4 * ml: {ensemble.predict(context)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complex Compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sophisticated model compositions\n",
    "\n",
    "# 1. N-gram with projection + LLM ensemble\n",
    "sophisticated = 0.3 * (ngram_3 @ recency) + 0.7 * (llm_technical | llm_ml)\n",
    "\n",
    "# 2. Cascading model with fallback\n",
    "from ngram_projections.algebra.combinators import cascade\n",
    "\n",
    "# Create a confidence-based cascade\n",
    "def confidence_cascade(models, context, threshold=0.3):\n",
    "    for model in models:\n",
    "        pred = model.predict(context)\n",
    "        if pred and max(pred.values()) > threshold:\n",
    "            return pred\n",
    "    return models[-1].predict(context)\n",
    "\n",
    "# Models in order of speed/confidence\n",
    "fast_model = ngram_2 @ recency\n",
    "medium_model = ngram_3 @ semantic  \n",
    "slow_model = 0.5 * llm_technical + 0.5 * llm_ml\n",
    "\n",
    "# Test cascade\n",
    "contexts = [\n",
    "    [\"the\", \"dog\"],\n",
    "    [\"neural\", \"network\"],\n",
    "    [\"unknown\", \"context\"],\n",
    "]\n",
    "\n",
    "print(\"Cascade model testing:\")\n",
    "for ctx in contexts:\n",
    "    result = confidence_cascade([fast_model, medium_model, slow_model], ctx)\n",
    "    print(f\"Context {ctx}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Functional Combinators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngram_projections.algebra.combinators import compose, ensemble, memoize\n",
    "\n",
    "# Compose functions for preprocessing\n",
    "def lowercase(tokens):\n",
    "    return [t.lower() for t in tokens]\n",
    "\n",
    "def limit_length(max_len):\n",
    "    def limiter(tokens):\n",
    "        return tokens[-max_len:]\n",
    "    return limiter\n",
    "\n",
    "def add_context_token(token):\n",
    "    def adder(tokens):\n",
    "        return [token] + tokens\n",
    "    return adder\n",
    "\n",
    "# Compose preprocessing pipeline\n",
    "preprocess = compose(lowercase, limit_length(5), add_context_token(\"<CTX>\"))\n",
    "\n",
    "test_input = [\"The\", \"Quick\", \"BROWN\", \"Fox\", \"Jumps\", \"Over\", \"The\", \"Lazy\", \"Dog\"]\n",
    "print(f\"Original: {test_input}\")\n",
    "print(f\"Preprocessed: {preprocess(test_input)}\\n\")\n",
    "\n",
    "# Ensemble with custom aggregation\n",
    "def weighted_avg(predictions, weights):\n",
    "    \"\"\"Custom aggregation for ensemble.\"\"\"\n",
    "    result = {}\n",
    "    for pred, w in zip(predictions, weights):\n",
    "        for token, prob in pred.items():\n",
    "            result[token] = result.get(token, 0) + w * prob\n",
    "    return result\n",
    "\n",
    "# Create ensemble\n",
    "models = [ngram_2, ngram_3, llm_technical]\n",
    "weights = [0.2, 0.5, 0.3]\n",
    "\n",
    "ensemble_model = ensemble(models, lambda preds: weighted_avg(preds, weights))\n",
    "\n",
    "# Memoize expensive computations\n",
    "expensive_model = memoize(0.3 * (ngram_3 @ semantic) + 0.7 * llm_ml)\n",
    "\n",
    "print(\"Testing memoized model:\")\n",
    "context = [\"machine\", \"learning\"]\n",
    "print(f\"First call: {expensive_model.predict(context)}\")\n",
    "print(f\"Second call (cached): {expensive_model.predict(context)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualization of Model Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how different models and mixtures behave\n",
    "\n",
    "def plot_predictions(models, context, title=\"Model Predictions\"):\n",
    "    \"\"\"Plot prediction distributions for multiple models.\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(models), figsize=(15, 4))\n",
    "    if len(models) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (name, model) in zip(axes, models):\n",
    "        pred = model.predict(context)\n",
    "        if pred:\n",
    "            tokens = list(pred.keys())[:10]  # Top 10\n",
    "            probs = [pred[t] for t in tokens]\n",
    "            \n",
    "            ax.bar(range(len(tokens)), probs)\n",
    "            ax.set_xticks(range(len(tokens)))\n",
    "            ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "            ax.set_ylabel('Probability')\n",
    "            ax.set_title(name)\n",
    "            ax.set_ylim([0, max(probs) * 1.2 if probs else 1])\n",
    "    \n",
    "    plt.suptitle(f\"{title}\\nContext: {' '.join(context)}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compare different models\n",
    "context = [\"neural\", \"network\"]\n",
    "\n",
    "models_to_compare = [\n",
    "    (\"N-gram\", ngram_3),\n",
    "    (\"N-gram + Recency\", ngram_3 @ recency),\n",
    "    (\"LLM\", llm_technical),\n",
    "    (\"Mixture\", 0.3 * ngram_3 + 0.7 * llm_technical),\n",
    "]\n",
    "\n",
    "plot_predictions(models_to_compare, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_model(model, contexts, runs=100):\n",
    "    \"\"\"Benchmark model performance.\"\"\"\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.time()\n",
    "        for ctx in contexts:\n",
    "            _ = model.predict(ctx)\n",
    "        times.append(time.time() - start)\n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "# Test contexts\n",
    "test_contexts = [\n",
    "    [\"the\", \"dog\"],\n",
    "    [\"neural\", \"network\", \"models\"],\n",
    "    [\"machine\", \"learning\", \"algorithms\", \"process\"],\n",
    "]\n",
    "\n",
    "# Models to benchmark\n",
    "models_to_benchmark = {\n",
    "    \"N-gram (2)\": ngram_2,\n",
    "    \"N-gram (3)\": ngram_3,\n",
    "    \"N-gram + Projection\": ngram_3 @ recency,\n",
    "    \"Simple Mixture\": ngram_2 + ngram_3,\n",
    "    \"Complex Mixture\": 0.3 * (ngram_3 @ recency) + 0.7 * llm_technical,\n",
    "}\n",
    "\n",
    "print(\"Performance Benchmarks (100 runs):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "results = {}\n",
    "for name, model in models_to_benchmark.items():\n",
    "    mean_time, std_time = benchmark_model(model, test_contexts, runs=100)\n",
    "    results[name] = (mean_time * 1000, std_time * 1000)  # Convert to ms\n",
    "    print(f\"{name:25} {mean_time*1000:.2f} ± {std_time*1000:.2f} ms\")\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "names = list(results.keys())\n",
    "means = [results[n][0] for n in names]\n",
    "stds = [results[n][1] for n in names]\n",
    "\n",
    "ax.bar(range(len(names)), means, yerr=stds, capsize=5)\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(names, rotation=45, ha='right')\n",
    "ax.set_ylabel('Time (ms)')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Real-World Example: Adaptive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveModel:\n",
    "    \"\"\"Model that adapts its mixture weights based on context.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model, adaptation_model, alpha=0.5):\n",
    "        self.base_model = base_model\n",
    "        self.adaptation_model = adaptation_model\n",
    "        self.alpha = alpha\n",
    "        self.history = []\n",
    "    \n",
    "    def predict(self, context):\n",
    "        # Adapt alpha based on context length\n",
    "        if len(context) < 3:\n",
    "            # Short context: rely more on base model\n",
    "            current_alpha = min(0.7, self.alpha * 1.5)\n",
    "        else:\n",
    "            # Long context: rely more on adaptation model  \n",
    "            current_alpha = max(0.3, self.alpha * 0.7)\n",
    "        \n",
    "        # Create dynamic mixture\n",
    "        mixture = current_alpha * self.base_model + (1 - current_alpha) * self.adaptation_model\n",
    "        \n",
    "        # Store for analysis\n",
    "        self.history.append({\n",
    "            'context_len': len(context),\n",
    "            'alpha': current_alpha,\n",
    "            'context': context\n",
    "        })\n",
    "        \n",
    "        return mixture.predict(context)\n",
    "    \n",
    "    def analyze_adaptation(self):\n",
    "        \"\"\"Analyze how the model has been adapting.\"\"\"\n",
    "        if not self.history:\n",
    "            print(\"No history available\")\n",
    "            return\n",
    "        \n",
    "        lengths = [h['context_len'] for h in self.history]\n",
    "        alphas = [h['alpha'] for h in self.history]\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.scatter(lengths, alphas, alpha=0.6)\n",
    "        plt.xlabel('Context Length')\n",
    "        plt.ylabel('Alpha (base model weight)')\n",
    "        plt.title('Adaptive Model Behavior')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "# Create and test adaptive model\n",
    "adaptive = AdaptiveModel(\n",
    "    base_model=ngram_2,\n",
    "    adaptation_model=ngram_3 @ semantic,\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "# Test with various context lengths\n",
    "test_cases = [\n",
    "    [\"the\"],\n",
    "    [\"the\", \"dog\"],\n",
    "    [\"the\", \"quick\", \"brown\"],\n",
    "    [\"neural\", \"network\", \"models\", \"learn\"],\n",
    "    [\"machine\", \"learning\", \"algorithms\", \"process\", \"natural\", \"language\"],\n",
    "]\n",
    "\n",
    "print(\"Adaptive Model Predictions:\")\n",
    "print(\"-\" * 50)\n",
    "for ctx in test_cases:\n",
    "    pred = adaptive.predict(ctx)\n",
    "    if pred:\n",
    "        top_token = max(pred.items(), key=lambda x: x[1])\n",
    "        print(f\"Context: {' '.join(ctx):30} -> {top_token[0]} ({top_token[1]:.3f})\")\n",
    "\n",
    "# Analyze adaptation behavior\n",
    "adaptive.analyze_adaptation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Continuous Learning Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousLearningModel:\n",
    "    \"\"\"Simulates continuous learning with n-gram updates.\"\"\"\n",
    "    \n",
    "    def __init__(self, static_llm, n=3, ngram_weight=0.3):\n",
    "        self.static_llm = static_llm\n",
    "        self.ngram = NGramModel(n=n)\n",
    "        self.ngram_weight = ngram_weight\n",
    "        self.updates = 0\n",
    "        self.performance_history = []\n",
    "    \n",
    "    def observe(self, text):\n",
    "        \"\"\"Observe new data and update n-gram model.\"\"\"\n",
    "        tokens = text.lower().split()\n",
    "        self.ngram.train(tokens)\n",
    "        self.updates += 1\n",
    "    \n",
    "    def predict(self, context):\n",
    "        \"\"\"Make prediction with current mixture.\"\"\"\n",
    "        mixture = self.ngram_weight * self.ngram + (1 - self.ngram_weight) * self.static_llm\n",
    "        return mixture.predict(context)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Evaluate current performance.\"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for text in test_data:\n",
    "            tokens = text.lower().split()\n",
    "            for i in range(2, len(tokens)):\n",
    "                context = tokens[:i]\n",
    "                target = tokens[i]\n",
    "                \n",
    "                pred = self.predict(context)\n",
    "                if pred and target in pred:\n",
    "                    correct += pred[target]\n",
    "                total += 1\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        self.performance_history.append((self.updates, accuracy))\n",
    "        return accuracy\n",
    "\n",
    "# Simulate continuous learning\n",
    "print(\"Simulating Continuous Learning:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize model\n",
    "cl_model = ContinuousLearningModel(\n",
    "    static_llm=llm_general,\n",
    "    n=3,\n",
    "    ngram_weight=0.3\n",
    ")\n",
    "\n",
    "# Streaming data (simulated)\n",
    "stream_data = [\n",
    "    \"The new research on quantum computing shows promise\",\n",
    "    \"Quantum algorithms can solve certain problems exponentially faster\",\n",
    "    \"Machine learning models require large amounts of data\",\n",
    "    \"Deep neural networks learn hierarchical representations\",\n",
    "    \"Quantum machine learning combines both paradigms\",\n",
    "]\n",
    "\n",
    "# Test data\n",
    "test_data = [\n",
    "    \"quantum computing research\",\n",
    "    \"machine learning algorithms\",\n",
    "    \"neural networks learn\",\n",
    "]\n",
    "\n",
    "# Initial evaluation\n",
    "initial_acc = cl_model.evaluate(test_data)\n",
    "print(f\"Initial accuracy: {initial_acc:.3f}\")\n",
    "\n",
    "# Continuous learning loop\n",
    "for i, data in enumerate(stream_data):\n",
    "    cl_model.observe(data)\n",
    "    acc = cl_model.evaluate(test_data)\n",
    "    print(f\"After update {i+1}: accuracy = {acc:.3f}\")\n",
    "\n",
    "# Plot learning curve\n",
    "if cl_model.performance_history:\n",
    "    updates, accuracies = zip(*cl_model.performance_history)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(updates, accuracies, 'o-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Number of Updates')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Continuous Learning Performance')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nFinal n-gram size: {len(cl_model.ngram.counts)} unique n-grams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Advanced Algebra: Custom Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom operators for specialized compositions\n",
    "\n",
    "class ContextAwareModel(LanguageModel):\n",
    "    \"\"\"Model that changes behavior based on context properties.\"\"\"\n",
    "    \n",
    "    def __init__(self, formal_model, casual_model):\n",
    "        self.formal_model = formal_model\n",
    "        self.casual_model = casual_model\n",
    "    \n",
    "    def predict(self, context):\n",
    "        # Detect formality from context\n",
    "        formal_indicators = {'research', 'algorithm', 'neural', 'quantum', 'model'}\n",
    "        casual_indicators = {'dog', 'cat', 'quick', 'lazy', 'brown'}\n",
    "        \n",
    "        formal_score = sum(1 for token in context if token in formal_indicators)\n",
    "        casual_score = sum(1 for token in context if token in casual_indicators)\n",
    "        \n",
    "        if formal_score > casual_score:\n",
    "            return self.formal_model.predict(context)\n",
    "        else:\n",
    "            return self.casual_model.predict(context)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ContextAware({self.formal_model} <-> {self.casual_model})\"\n",
    "\n",
    "# Create context-aware model\n",
    "formal = 0.2 * ngram_3 + 0.8 * llm_technical\n",
    "casual = 0.6 * ngram_2 + 0.4 * llm_general\n",
    "\n",
    "context_aware = ContextAwareModel(formal, casual)\n",
    "\n",
    "# Test with different contexts\n",
    "test_contexts = [\n",
    "    [\"neural\", \"network\", \"algorithm\"],  # Formal\n",
    "    [\"the\", \"dog\", \"ran\"],  # Casual\n",
    "    [\"quantum\", \"computing\", \"research\"],  # Formal\n",
    "    [\"quick\", \"brown\", \"fox\"],  # Casual\n",
    "]\n",
    "\n",
    "print(\"Context-Aware Model Predictions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for ctx in test_contexts:\n",
    "    pred = context_aware.predict(ctx)\n",
    "    if pred:\n",
    "        top_tokens = sorted(pred.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        print(f\"Context: {' '.join(ctx):25}\")\n",
    "        print(f\"  Top predictions: {', '.join([f'{t}({p:.2f})' for t, p in top_tokens])}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate best practices for model composition\n",
    "\n",
    "print(\"Best Practices for Algebraic Model Composition\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "\n",
    "# 1. Start simple\n",
    "print(\"1. Start Simple:\")\n",
    "simple = ngram_3 + llm_general\n",
    "print(f\"   Basic mixture: ngram + llm\")\n",
    "print()\n",
    "\n",
    "# 2. Add projections for context focus\n",
    "print(\"2. Add Projections:\")\n",
    "focused = (ngram_3 @ recency) + llm_general\n",
    "print(f\"   With projection: (ngram @ recency) + llm\")\n",
    "print()\n",
    "\n",
    "# 3. Use weights for fine-tuning\n",
    "print(\"3. Fine-tune with Weights:\")\n",
    "weighted = 0.3 * (ngram_3 @ recency) + 0.7 * llm_general\n",
    "print(f\"   Weighted: 0.3 * (ngram @ recency) + 0.7 * llm\")\n",
    "print()\n",
    "\n",
    "# 4. Compose projections for sophisticated behavior\n",
    "print(\"4. Compose Projections:\")\n",
    "sophisticated = ngram_3 @ (recency >> semantic)\n",
    "print(f\"   Composed: ngram @ (recency >> semantic)\")\n",
    "print()\n",
    "\n",
    "# 5. Use ensembles for robustness\n",
    "print(\"5. Build Ensembles:\")\n",
    "robust = (ngram_2 | ngram_3) + (llm_general | llm_technical)\n",
    "print(f\"   Ensemble: (ngram_2 | ngram_3) + (llm_general | llm_technical)\")\n",
    "print()\n",
    "\n",
    "# 6. Memoize expensive operations\n",
    "print(\"6. Optimize with Memoization:\")\n",
    "optimized = memoize(0.3 * (ngram_3 @ semantic) + 0.7 * llm_ml)\n",
    "print(f\"   Cached: memoize(expensive_mixture)\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"• Algebraic operations make complex models readable\")\n",
    "print(\"• Projections focus context for better predictions\")\n",
    "print(\"• Mixtures combine strengths of different models\")\n",
    "print(\"• Continuous learning via n-gram updates is efficient\")\n",
    "print(\"• Composition enables sophisticated behavior from simple parts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}